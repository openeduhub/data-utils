:PROPERTIES:
:header-args: :results verbatim :exports both :session demo.py :async yes :var foo=imports
:END:
#+title: Data Retrieval Utilities
#+EXPORT_EXCLUDE_TAGS: noexport

A Python library that aids in retrieving and processing large data dumps from [[https://github.com/elasticsearch-dump/elasticsearch-dump][elasticdump]], primarily for the purpose of training and evaluation of AI tools for the automatic generation of metadata.

There are two primary modules of this library:
- ~its_data.fetch~, which aids in downloading ~elasticdump~ data dumps and can turn them into ~pandas~ data frames.
  Secondarily, it implements some functionality for retrieving labels or other metadata for URIs and SKOS vocabularies.
- ~its_data.default_pipelines~, which collects various useful pipelines, such as ~its_data.default_pipelines.flat_classification~, that turn the data dumps into more readily usable forms whilst applying some light pre-processing.

For the purposes of downloading data dumps, this library also offers a CLI tool: ~download-data~.

For a more complete, technical documentation, see [[https://openeduhub.github.io/data-utils/]].

* Installation
:PROPERTIES:
:header-args: :results verbatim :exports both :session no 
:END:

** Nix Flakes (recommended)

When using / installing this library through ~Nix Flakes~, the only requirement is having installed [[https://nixos.org/download][Nix]] with ~Flakes~ support.

*** Running the CLI

The CLI can be run directly through ~Nix~'s CLI:
#+begin_src shell
nix run "github:openeduhub/data-utils#download-data" -- <optional arguments> <url>
#+end_src

Example:
#+begin_src shell
nix run "github:openeduhub/data-utils#download-data" -- --help
#+end_src

#+RESULTS:
#+begin_example
usage: download-data [-h] [-i INPUT_FILE] [-u USERNAME] [-p PASSWORD]
                     [-o OUTPUT_FILE] [--skip-if-exists] [--no-delete-archive]
                     [--version]
                     url

positional arguments:
  url                   The (base) URL from which to download the data dump.

options:
  -h, --help            show this help message and exit
  -i INPUT_FILE, --input-file INPUT_FILE
                        The name of the file from the URL to be downloaded. It
                        is assumed that this file is accessible through
                        <url/target-file>.
  -u USERNAME, --username USERNAME
                        The username to use when providing authentication
                        details. Optional unless a password is provided.
  -p PASSWORD, --password PASSWORD
                        The password to use when providing authentication
                        details. Optional unless a username is provided.
  -o OUTPUT_FILE, --output-file OUTPUT_FILE
                        The path to the output file. If a directory, save the
                        (decompressed) target file to this directory.
  --skip-if-exists      Skip files that already exist.
  --no-delete-archive   Do not delete the original archive if it was
                        compressed.
  --version             show program's version number and exit
#+end_example

*** As Python library, Option A: using lib

To use the library as part of a bigger Python environment, e.g. to use in another application, first include this ~Flake~ within the inputs of your own =flake.nix=:
#+begin_src nix
# flake.nix
{
  inputs = {
    data-utils.url = "/home/yusu/work/ITsJointly/projects/its_data";
  };
}
#+end_src

Then, you can reference the Python library with the ~data-utils.lib.${system}.data-utils~ attribute. This requires a reference to the Python packages that the library shall be built with, e.g.
#+begin_src nix
# flake.nix
{
  outputs = { self, nixpkgs, ... }:
    let
      system = ...;
      pkgs = nixpkgs.legacyPackages.${system};
      my-python = pkgs.python3.withPackages (py-pkgs: [
        # example python libraries to include
        py-pkgs.pandas
        py-pkgs.numpy
        # include data-utils library
        (self.inputs.data-utils.lib.data-utils py-pkgs)
      ]);
    in
      {...};
}
#+end_src

*** As Python library, Option B: using overlays

Just like with option A, include this ~Flake~ within your own:
#+begin_src nix
# flake.nix
{
  inputs = {
    data-utils.url = "/home/yusu/work/ITsJointly/projects/its_data";
  };
}
#+end_src

Then, when importing ~nixpkgs~, you can apply the provided overlay, which in turn allows you reference to ~data-utils~ just like any other Python library:
#+begin_src nix
# flake.nix
{
  outputs = { self, nixpkgs, ... }:
    let
      system = ...;
      pkgs = nixpkgs.legacyPackages.${system}.extend
        self.inputs.data-utils.overlays.default;
      my-python = pkgs.python3.withPackages (py-pkgs: [
        # example python libraries to include
        py-pkgs.pandas
        py-pkgs.numpy
        # include data-utils library
        py-pkgs.data-utils
      ]);
    in
      {...};
}
#+end_src


** Through pip

This library should also be installable through Python's ~pip~. Simply running ~pip install~, i.e.
#+begin_src shell
pip install <this repository>
#+end_src
should be sufficient. However, this method of installation is untested.

* Usage

** Downloading Data

The probably most common way of downloading data is through the CLI by downloading a specific file from a URL whilst specifying a username and password for authentication. This can be done through:

#+begin_src shell
download-data -u <USERNAME> -p <PASSWORD> -i <FILE_NAME> -o <OUTPUT_FILE_PATH> <URL>
#+end_src

or, by using ~nix run~:

#+begin_src shell
nix run github:openeduhub/data-utils#download-data -- -u <USERNAME> -p <PASSWORD> -i <FILE_NAME> -o <OUTPUT_FILE_PATH> <URL>
#+end_src

Alternatively, we can download the data within a Python script:
#+begin_src python
from its_data.fetch import fetch

downloaded_file_path = fetch(
    base_url=URL,
    target_file=FILE_NAME,
    output_dir=OUTPUT_FILE_DIR, # optional
    output_file=OUTPUT_FILE_NAME, # optional
    username=USERNAME,
    password=PASSWORD,
)
#+end_src

If the target data file is detected to be compressed, i.e. its file-name ends on =.gz=, it will also be automatically be decompressed.

** Processing Pipelines

The default pipelines are intended to immediately obtain all relevant, lightly pre-processed data in formats that make sense for the given information. They remove a lot of redundant work, such as converting labels into boolean arrays or pulling labels from controlled vocabularies / URIs.

Imports of modules / functions that will be used further below.
#+name: imports
#+begin_src python :var foo=""
from pathlib import Path
from pprint import pprint

import numpy as np

import its_data.defaults as defaults
import its_data.filters as filters
from its_data.data import Data_Point, get_terminal_in
from its_data.default_pipelines.data import (subset_categories,
                                               subset_data_points,
                                               Processed_Data,
                                               BoW_Data)
from its_data.default_pipelines.flat_classification import generate_data
from its_data.defaults import Fields
#+end_src

#+RESULTS: imports

*** Basic Example: Non-Hierarchical Metadata

The ~its_data.default_pipelines.flat_classification.generate_data~ function is intended to obtain data together with any number of metadata fields that are not hierarchically organized (note that we can still apply it on hierarchical data, but the hierarchy will be discarded in the process).

After having downloaded the elasticdump json file to =~/data.json= we can directly use the ~generate_data~ function:
#+begin_src python :results silent
data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[
        "properties.ccm:educationalcontext",
        "properties.ccm:taxonid",
    ],
    max_len=1000,
)
#+end_src

The object ~data~ now contains all of the commonly used information we may need for classification tasks or their evaluation:
- =raw_texts=: The title, concatenated with the description (and separated by a =\n=).
- =ids=: The internal unique identifiers of the materials.
- =editor_arr=: A Boolean array containing information about whether each material belongs to the "Redaktionsbuffet" (i.e. its quality has been confirmed by an editor).
- =target_data=: Information about each selected metadata field:
  - =arr=: The Boolean matrix mapping each material to all of its relevant categories.
    When multiple assignment per material is not possible, this is equivalent to a one-hot-encoding.
  - =uris=: The URIs that correspond to each matrix column.
  - =labels=: The labels of the URIs.
  - =in_test_set=: Whether each data point belongs to the test data set for this metadatum.
    
#+begin_src python :session demo.py :exports results :results output
print(f"{data.raw_texts[0]=}\n")
print(f"{data.ids[:5]=}\n")
print(f"{data.editor_arr.shape=}\n")
print(f"{data.target_data.keys()=}\n")
print(f"{data.target_data['properties.ccm:taxonid'].uris[:5]=}\n")
print(f"{data.target_data['properties.ccm:taxonid'].labels[:5]=}\n")
print(f"{data.target_data['properties.ccm:taxonid'].arr.shape=}\n")
print(f"{data.target_data['properties.ccm:taxonid'].in_test_set.shape=}\n")
print(f"{data.target_data['properties.ccm:educationalcontext'].arr.shape=}\n")
#+end_src

#+RESULTS:
#+begin_example
data.raw_texts[0]='Eiszeit mit unserem neuen Freund Puck | Sport macht Spaß\nHeidewitzka, heute wird es rasant bei #SportmachtSpaß! Inga, Stefan und der Albatros laden euch in dieser Folge zum Eishockeyspielen ein, und dafür brauchen wir natürlich ein Spielgerät, nämlich den Puck. Wir zeigen euch, wie ihr den mit ein paar Materialien von zuhause selbst basteln könnt. Außerdem ist es wichtig, dass ihr ein bisschen Platz immer Zimmer schafft und gut auf Schienbeine, nackte Füße, schöne Vasen oder Haustiere aufpasst, weil die Pucks ziemlich schnell werden können. Bevor es aufs Eis geht, stärken wir uns dann erst einmal mit einem Spiel namens Bohnenfrühstück. Danach geht es aber auch schon los mit unserem Puck: Mit ihm zeichnen wir Figuren und Zahlen aufs Eis und zielen gemeinsam auf Kegel. Außerdem haben wir wieder ein Spiel fürs Wochenende im Gepäck: Es trägt den Namen Streuselkuchen und verlangt ein gutes Händchen. Seid gespannt!\n\nUnser Mitmach-Sportprogramm für Kitakinder gibt es jeden Samstag um 9 Uhr mit einer neuen Folge auf unserem YouTube-Kanal. Um 10 Uhr erscheint zudem eine Sendung „Sport macht Spaß“ für die Grundschule. \n\nWir freuen uns auf eure Ideen und Kommentare! Schreibt uns einfach eine Mail an sportmachtspass@albaberlin.de – hier könnt ihr euch auch für unseren monatlichen Newsletter mit allen Infos rund um „Sport macht Spaß“ anmelden.\n_________________ \n\nWas erwartet euch in dieser Folge? \n\n00:00 Begrüßung\n05:18 Bohnenfrühstück\n12:40 Auf die Kegel! Fertig! Schuss!\n19:54 Streuselkuchen\n__________________ \n\n„Sport macht Spaß“ ist Teil des großen digitalen Sportprogramms „Sport digital – Mehr Bewegung im Quartier“, das wir gemeinsam mit dem Bundesministerium des Innern, für Bau und Heimat (BMI) ins Leben gerufen haben. Wir bieten euch dabei kontinuierlich digitale Sportstunden für Kita- und Schulkinder hier bei YouTube an, es wird eine Mediathek mit Bildungs- und Informationsvideos aufgebaut, und es gibt Fortbildungen für pädagogische Fachkräfte im Netz und direkt in den Kiezen. Unter https://www.albaberlin.de/jugend/kita-schule-uni/sport-digital/ und https://www.miteinander-im-quartier.de/ findet ihr alle Infos zu unserem Modellprojekt, das im Rahmen der ressortübergreifenden Strategie „Soziale Stadt – Nachbarschaften stärken, miteinander im Quartier“ gefördert wird.\n\nRedaktion: Lena Flöttmann, Christoph Nicol & das ALBA-Kitateam | Kamera: Kai Rostásy & Andreas Reuther | Regie: Jörg Diernberger | Schnitt: Marcus Wojatschke\n\n#ALBA #Sportstunde #Sport #Bewegung #Zuhause #Kita #Kitasport #Kindersport #Mitmachsport #Laufspiel #Einführung #Sportgerät #Schläger #Zielen #Eishockey #Puck'

data.ids[:5]=array(['7932dcd4-5d9e-4c24-bbcc-efdfa47d2d78',
       'fa74d4b0-49eb-4525-8be6-8f7af9916285',
       'da1213d8-4f30-4a18-b54d-61f057e32f22',
       'a7467ea5-21b2-4449-adf1-80b7dbdaca92',
       'b1aa975c-352f-447c-9f1d-704dcbfbf423'], dtype=object)

data.editor_arr.shape=(924,)

data.target_data.keys()=dict_keys(['properties.ccm:educationalcontext', 'properties.ccm:taxonid'])

data.target_data['properties.ccm:taxonid'].uris[:5]=array(['http://w3id.org/openeduhub/vocabs/discipline/46014',
       'http://w3id.org/openeduhub/vocabs/discipline/48005',
       'http://w3id.org/openeduhub/vocabs/discipline/320',
       'http://w3id.org/openeduhub/vocabs/discipline/240',
       'http://w3id.org/openeduhub/vocabs/discipline/28002'], dtype='<U50')

data.target_data['properties.ccm:taxonid'].labels[:5]=array(['Astronomie', 'Gesellschaftskunde', 'Informatik', 'Geschichte',
       'Deutsch als Zweitsprache'], dtype='<U28')

data.target_data['properties.ccm:taxonid'].arr.shape=(924, 39)

data.target_data['properties.ccm:taxonid'].in_test_set.shape=(924,)

data.target_data['properties.ccm:educationalcontext'].arr.shape=(924, 10)
#+end_example

In the long run, typing the full identifiers for the metadata fields can be error prone and tiring. Thus, we provide an ~Enum~ that contains the most common fields:
#+begin_src python :results output :exports results
print(f"{data.target_data[Fields.EDUCATIONAL_CONTEXT.value].uris[:5]=}\n")
#+end_src

#+RESULTS:
: ced189fc459540b0a24f05711edcaea1

*** Getting Readable Category Labels

If the values assigned to a targeted field are URIs that link back to their controlled vocabularies, the ~generate_data~ function will automatically try to look up the preferred label (default: =prefLabel.de=):

#+begin_src python
data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[Fields.TAXONID.value],
    max_len=1000,
    use_defaults=False,
)
#+end_src

#+RESULTS:

#+begin_src python :exports results :results output
pprint(
    {
        uri: label
        for uri, label in zip(
            data.target_data[Fields.TAXONID.value].uris[:10],
            data.target_data[Fields.TAXONID.value].labels,
        )
    }
)
#+end_src

#+RESULTS:
#+begin_example
{'http://w3id.org/openeduhub/vocabs/discipline/080': 'Biologie',
 'http://w3id.org/openeduhub/vocabs/discipline/160': 'Ethik',
 'http://w3id.org/openeduhub/vocabs/discipline/20002': 'Französisch',
 'http://w3id.org/openeduhub/vocabs/discipline/240': 'Geschichte',
 'http://w3id.org/openeduhub/vocabs/discipline/260': 'Gesundheit',
 'http://w3id.org/openeduhub/vocabs/discipline/28002': 'Deutsch als Zweitsprache',
 'http://w3id.org/openeduhub/vocabs/discipline/320': 'Informatik',
 'http://w3id.org/openeduhub/vocabs/discipline/46014': 'Astronomie',
 'http://w3id.org/openeduhub/vocabs/discipline/48005': 'Gesellschaftskunde',
 'http://w3id.org/openeduhub/vocabs/discipline/900': 'Medienbildung'}
#+end_example

Additionally, we can provide a map from metadata field to SKOS vocabulary. For all fields where this is provided, this vocabulary will be used instead of dynamically looking up the label.
This has the advantage of being much faster (only one network access instead of one per unique value) and being able to support URIs that do not directly link back to their controlled vocabularies.
#+begin_src python 
data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[Fields.TAXONID.value],
    max_len=1000,
    use_defaults=False,
    skos_urls={Fields.TAXONID.value: "https://vocabs.openeduhub.de/w3id.org/openeduhub/vocabs/discipline/index.json"},
)
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
pprint(
    {
        uri: label
        for uri, label in zip(
            data.target_data[Fields.TAXONID.value].uris[:10],
            data.target_data[Fields.TAXONID.value].labels,
        )
    }
)
#+end_src

#+RESULTS:
#+begin_example
{'http://w3id.org/openeduhub/vocabs/discipline/080': 'Biologie',
 'http://w3id.org/openeduhub/vocabs/discipline/160': 'Ethik',
 'http://w3id.org/openeduhub/vocabs/discipline/20002': 'Französisch',
 'http://w3id.org/openeduhub/vocabs/discipline/240': 'Geschichte',
 'http://w3id.org/openeduhub/vocabs/discipline/260': 'Gesundheit',
 'http://w3id.org/openeduhub/vocabs/discipline/28002': 'Deutsch als Zweitsprache',
 'http://w3id.org/openeduhub/vocabs/discipline/320': 'Informatik',
 'http://w3id.org/openeduhub/vocabs/discipline/46014': 'Astronomie',
 'http://w3id.org/openeduhub/vocabs/discipline/48005': 'Gesellschaftskunde',
 'http://w3id.org/openeduhub/vocabs/discipline/900': 'Medienbildung'}
#+end_example

Some controlled vocabularies are already defined in ~its_data.defaults.skos_urls~:
#+begin_src python :results output :exports results 
pprint(defaults.skos_urls)
#+end_src

#+RESULTS:
: {'properties.ccm:curriculum': 'https://vocabs.openeduhub.de/w3id.org/openeduhub/vocabs/oeh-topics/5e40e372-735c-4b17-bbf7-e827a5702b57.json',
:  'properties.ccm:educationalcontext': 'https://vocabs.openeduhub.de/w3id.org/openeduhub/vocabs/educationalContext/index.json',
:  'properties.ccm:educationalintendedenduserrole': 'https://vocabs.openeduhub.de/w3id.org/openeduhub/vocabs/intendedEndUserRole/index.json',
:  'properties.ccm:fskRating': 'https://vocabs.openeduhub.de/w3id.org/openeduhub/vocabs/fskRating/index.json',
:  'properties.ccm:oeh_lrt': 'https://vocabs.openeduhub.de/w3id.org/openeduhub/vocabs/new_lrt/index.json',
:  'properties.ccm:taxonid': 'https://vocabs.openeduhub.de/w3id.org/openeduhub/vocabs/discipline/index.json'}

*** Fixing Inconsistent Categories

The =remapped_values= argument allows us to provide a dictionary for any number of selected metadata fields, defining which original values shall be mapped to which new ones. For example, this may be used to unify the language codes:

#+begin_src python
example_remapped_values = {
    Fields.LANGUAGE.value: {
        "de_DE": "de",
        "de_AT": "de",
        "DE": "de",
        "de-DE": "de",
        "Deutsch": "de",
        "en-US-LEARN": "en",
        "en_US": "en",
        "en_GB": "en",
        "hu_HU": "hu",
        "es_CR": "es",
        "es_ES": "es",
        "es_AR": "es",
        "fr_FR": "fr",
        "tr_TR": "tr",
        "latin": "la",
    }
}
#+end_src

#+RESULTS:

Additionally, specific values can be dropped entirely (but not the corresponding entry) with the =dropped_values= argument, which takes a dictionary mapping metadata field to a collection of strings that shall be dropped.

Note that for some metadata fields, there already exists some defaults that may be used (see [[file:its_data/defaults.py][defaults.py]]). These are loaded automatically when the =use_defaults= argument is set to =True= (default).

Example without defaults:
#+begin_src python
data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[Fields.TAXONID.value],
    use_defaults=False,
)
#+end_src

#+RESULTS:

#+begin_src python :exports results :results output
pprint(data.target_data[Fields.TAXONID.value].arr.shape)
pprint(data.target_data[Fields.TAXONID.value].uris)
#+end_src

#+RESULTS:
#+begin_example
(295814, 86)
array(['http://w3id.org/openeduhub/vocabs/discipline/640', '',
       'http://w3id.org/openeduhub/vocabs/discipline/20041',
       'https://w3id.org/openeduhub/vocabs/discipline/720',
       'http://w3id.org/openeduhub/vocabs/discipline/48005',
       'http://w3id.org/openeduhub/vocabs/discipline/28002',
       'http://w3id.org/openeduhub/vocabs/discipline/900',
       'http://w3id.org/openeduhub/vocabs/discipline/04014',
       'http://w3id.org/openeduhub/vocabs/discipline/080',
       'http://w3id.org/openeduhub/vocabs/discipline/120',
       'http://w3id.org/openeduhub/vocabs/discipline/450',
       'http://w3id.org/openeduhub/vocabs/discipline/720',
       'http://w3id.org/openeduhub/vocabs/discipline/060',
       'http://w3id.org/openeduhub/vocabs/discipline/420',
       'http://w3id.org/openeduhub/vocabs/discipline/44007',
       'http://w3id.org/openeduhub/vocabs/discipline/niederdeutsch',
       'http://w3id.org/openeduhub/vocabs/discipline/Inhalte',
       'http://w3id.org/openeduhub/vocabs/discipline/460',
       'http://w3id.org/openeduhub/vocabs/discipline/oeh04010',
       'http://w3id.org/openeduhub/vocabs/discipline/???',
       'http://w3id.org/openeduhub/vocabs/discipline/400',
       'http://w3id.org/openeduhub/vocabs/discipline/340',
       'http://w3id.org/openeduhub/vocabs/discipline/20009',
       'http://w3id.org/openeduhub/vocabs/discipline/Spanisch',
       'http://w3id.org/openeduhub/vocabs/discipline/020',
       'http://w3id.org/openeduhub/vocabs/discipline/Religion',
       'http://w3id.org/openeduhub/vocabs/discipline/Geschichte',
       'http://w3id.org/openeduhub/vocabs/discipline/04011',
       'http://w3id.org/openeduhub/vocabs/discipline/50005',
       'http://w3id.org/openeduhub/vocabs/discipline/560',
       'http://w3id.org/openeduhub/vocabs/discipline/Physik',
       'http://w3id.org/openeduhub/vocabs/discipline/Deutsch',
       'http://w3id.org/openeduhub/vocabs/discipline/04013',
       'http://w3id.org/openeduhub/vocabs/discipline/999',
       'http://w3id.org/openeduhub/vocabs/discipline/04012',
       'https://w3id.org/openeduhub/vocabs/discipline/320',
       'http://w3id.org/openeduhub/vocabs/discipline/04003',
       'http://w3id.org/openeduhub/vocabs/discipline/44006',
       'http://w3id.org/openeduhub/vocabs/discipline/510',
       'http://w3id.org/openeduhub/vocabs/discipline/28010',
       'http://w3id.org/openeduhub/vocabs/discipline/Englisch',
       'http://w3id.org/openeduhub/vocabs/discipline/46014',
       'http://w3id.org/openeduhub/vocabs/discipline/320',
       'http://w3id.org/openeduhub/vocabs/discipline/240',
       'http://w3id.org/openeduhub/vocabs/discipline/260',
       'http://w3id.org/openeduhub/vocabs/discipline/160',
       'http://w3id.org/openeduhub/vocabs/discipline/680',
       'http://w3id.org/openeduhub/vocabs/discipline/20003',
       'http://w3id.org/openeduhub/vocabs/discipline/04009',
       'http://w3id.org/openeduhub/vocabs/discipline/20002',
       'http://w3id.org/openeduhub/vocabs/discipline/20008',
       'http://w3id.org/openeduhub/vocabs/discipline/Darstellendes-Spiel',
       'http://w3id.org/openeduhub/vocabs/discipline/20001',
       'http://w3id.org/openeduhub/vocabs/discipline/04006',
       'http://w3id.org/openeduhub/vocabs/discipline/oeh01',
       'http://w3id.org/openeduhub/vocabs/discipline/Deutsch als Zweitsprache',
       'http://w3id.org/openeduhub/vocabs/discipline/220',
       'http://w3id.org/openeduhub/vocabs/discipline/480',
       'http://w3id.org/openeduhub/vocabs/discipline/20006',
       'http://w3id.org/openeduhub/vocabs/discipline/380',
       'http://w3id.org/openeduhub/vocabs/discipline/700',
       'http://w3id.org/openeduhub/vocabs/discipline/20005',
       'http://w3id.org/openeduhub/vocabs/discipline/04007',
       'http://w3id.org/openeduhub/vocabs/discipline/600',
       'http://w3id.org/openeduhub/vocabs/discipline/04002',
       'http://w3id.org/openeduhub/voca


bs/discipline/12002',
       'http://w3id.org/openeduhub/vocabs/discipline/100',
       'http://w3id.org/openeduhub/vocabs/discipline/Mathematik',
       'http://w3id.org/openeduhub/vocabs/discipline/Pädagogik',
       'http://w3id.org/openeduhub/vocabs/discipline/44099',
       'http://w3id.org/openeduhub/vocabs/discipline/440',
       'http://w3id.org/openeduhub/vocabs/discipline/04005',
       'http://w3id.org/openeduhub/vocabs/discipline/660',
       'https://w3id.org/openeduhub/vocabs/discipline/380',
       'http://w3id.org/openeduhub/vocabs/discipline/04001',
       'http://w3id.org/openeduhub/vocabs/discipline/Informatik',
       'https://w3id.org/openeduhub/vocabs/discipline/460',
       'http://w3id.org/openeduhub/vocabs/discipline/040',
       'http://w3id.org/openeduhub/vocabs/discipline/520',
       'https://w3id.org/openeduhub/vocabs/discipline/120',
       'http://w3id.org/openeduhub/vocabs/discipline/20007',
       'http://w3id.org/openeduhub/vocabs/discipline/Geografie',
       'http://w3id.org/openeduhub/vocabs/discipline/72001',
       'http://w3id.org/openeduhub/vocabs/discipline/50001',
       'http://w3id.org/openeduhub/vocabs/discipline/20004',
       'http://w3id.org/openeduhub/vocabs/discipline/64018'], dtype='<U69')
#+end_example

Example with defaults:
#+begin_src python
data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[Fields.TAXONID.value],
    use_defaults=True,
)
#+end_src

#+RESULTS:

#+begin_src python :exports results :results output
pprint(data.target_data[Fields.TAXONID.value].arr.shape)
pprint(data.target_data[Fields.TAXONID.value].uris)
#+end_src

#+RESULTS:
#+begin_example
(158404, 66)
array(['http://w3id.org/openeduhub/vocabs/discipline/640',
       'http://w3id.org/openeduhub/vocabs/discipline/20041',
       'http://w3id.org/openeduhub/vocabs/discipline/48005',
       'http://w3id.org/openeduhub/vocabs/discipline/28002',
       'http://w3id.org/openeduhub/vocabs/discipline/900',
       'http://w3id.org/openeduhub/vocabs/discipline/04014',
       'http://w3id.org/openeduhub/vocabs/discipline/080',
       'http://w3id.org/openeduhub/vocabs/discipline/120',
       'http://w3id.org/openeduhub/vocabs/discipline/450',
       'http://w3id.org/openeduhub/vocabs/discipline/720',
       'http://w3id.org/openeduhub/vocabs/discipline/060',
       'http://w3id.org/openeduhub/vocabs/discipline/420',
       'http://w3id.org/openeduhub/vocabs/discipline/44007',
       'http://w3id.org/openeduhub/vocabs/discipline/niederdeutsch',
       'http://w3id.org/openeduhub/vocabs/discipline/460',
       'http://w3id.org/openeduhub/vocabs/discipline/oeh04010',
       'http://w3id.org/openeduhub/vocabs/discipline/400',
       'http://w3id.org/openeduhub/vocabs/discipline/340',
       'http://w3id.org/openeduhub/vocabs/discipline/20009',
       'http://w3id.org/openeduhub/vocabs/discipline/020',
       'http://w3id.org/openeduhub/vocabs/discipline/04011',
       'http://w3id.org/openeduhub/vocabs/discipline/50005',
       'http://w3id.org/openeduhub/vocabs/discipline/560',
       'http://w3id.org/openeduhub/vocabs/discipline/04013',
       'http://w3id.org/openeduhub/vocabs/discipline/999',
       'http://w3id.org/openeduhub/vocabs/discipline/04012',
       'http://w3id.org/openeduhub/vocabs/discipline/04003',
       'http://w3id.org/openeduhub/vocabs/discipline/44006',
       'http://w3id.org/openeduhub/vocabs/discipline/510',
       'http://w3id.org/openeduhub/vocabs/discipline/28010',
       'http://w3id.org/openeduhub/vocabs/discipline/46014',
       'http://w3id.org/openeduhub/vocabs/discipline/320',
       'http://w3id.org/openeduhub/vocabs/discipline/240',
       'http://w3id.org/openeduhub/vocabs/discipline/260',
       'http://w3id.org/openeduhub/vocabs/discipline/160',
       'http://w3id.org/openeduhub/vocabs/discipline/680',
       'http://w3id.org/openeduhub/vocabs/discipline/20003',
       'http://w3id.org/openeduhub/vocabs/discipline/04009',
       'http://w3id.org/openeduhub/vocabs/discipline/20002',
       'http://w3id.org/openeduhub/vocabs/discipline/20008',
       'http://w3id.org/openeduhub/vocabs/discipline/20001',
       'http://w3id.org/openeduhub/vocabs/discipline/04006',
       'http://w3id.org/openeduhub/vocabs/discipline/oeh01',
       'http://w3id.org/openeduhub/vocabs/discipline/220',
       'http://w3id.org/openeduhub/vocabs/discipline/480',
       'http://w3id.org/openeduhub/vocabs/discipline/20006',
       'http://w3id.org/openeduhub/vocabs/discipline/380',
       'http://w3id.org/openeduhub/vocabs/discipline/700',
       'http://w3id.org/openeduhub/vocabs/discipline/20005',
       'http://w3id.org/openeduhub/vocabs/discipline/04007',
       'http://w3id.org/openeduhub/vocabs/discipline/600',
       'http://w3id.org/openeduhub/vocabs/discipline/04002',
       'http://w3id.org/openeduhub/vocabs/discipline/12002',
       'http://w3id.org/openeduhub/vocabs/discipline/100',
       'http://w3id.org/openeduhub/vocabs/discipline/44099',
       'http://w3id.org/openeduhub/vocabs/discipline/440',
       'http://w3id.org/openeduhub/vocabs/discipline/04005',
       'http://w3id.org/openeduhub/vocabs/discipline/660',
       'http://w3id.org/openeduhub/vocabs/discipline/04001',
       'http://w3id.org/openeduhub/vocabs/discipline/040',
       'http://w3id.org/openeduhub/vocabs/discipline/520',
       'http://w3id.org/openeduhub/vocabs/discipline/20007',
       'http://w3id.org/openeduhub/vocabs/discipline/72001',
       'http://w3id.org/openeduhub/vocabs/discipline/50001',
       'http://w3id.org/openeduhub/vocabs/discipline/20004',
       'http://w3id.org/openeduhub/vocabs/discipline/64018'], dtype='<U58')
#+end_example

*** Filtering out Entries

In addition to modifying categories, we can also define arbitrary rules that let us drop data points before they have even been fully processed. This can be used, for example, for filtering out data that is not of sufficient quality or that does not fulfill certain conditions.

To add such rules, use the ~filters~ keyword-argument of ~generate_data~:
#+begin_src python
def my_filter(entry: Data_Point) -> bool:
    description = get_terminal_in(
        entry,
        Fields.DESCRIPTION.value.split("."),
    )
    if description is None:
        return False
    # the description field is multi-valued
    return len(description[0]) > 5


data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[Fields.TAXONID.value],
    max_len=1000,
    use_defaults=False,
    filters=[my_filter],
)
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
print("Minimum text length:", min(len(text) for text in data.raw_texts))
#+end_src

#+RESULTS:
: Minimum text length: 19

To simplify the process of defining such filter functions, the ~its_data.filters~ module provides various helper functions. Especially useful here is ~get_filter_with_basic_predicate~, which creates a filter from a basic predicate function and a reference to the field to apply it to. /(Basic predicate functions are functions that map strings, floats, integers or None-values to a Boolean)./
#+begin_src python :results silent
my_filter2 = filters.get_filter_with_basic_predicate(
    lambda x: x is not None and len(x) > 5,
    Fields.DESCRIPTION.value,
    multi_value_semantics=any, # doesn't matter here; we only ever have one description
)

data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[Fields.TAXONID.value],
    max_len=1000,
    use_defaults=False,
    filters=[my_filter2],
)
#+end_src

#+begin_src python :results output :exports results
print("Minimum text length:", min(len(text) for text in data.raw_texts))
#+end_src

#+RESULTS:
: Minimum text length: 19

For more examples on how to define filter functions, see [[file:its_data/filters.py][its_data.filters]].

*** Dropping Categories and Data-Points using Global Information (e.g. Support)

While the ~generate_data~ function does not directly support filtering mechanisms that rely on information that is only present once the entire data set is loaded, we provide some utility functions to easily deal with such tasks after the data has been generated.

- ~its_data.default_pipelines.data.subset_data_points~ allows for dropping or sorting data points
- ~its_data.default_pipelines.data.subset_categories~ allows for dropping or sorting categories

**** Example: Dropping Categories with low Support

Load the data and calculate the initial support:
#+begin_src python
data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[
        Fields.TAXONID.value,
        Fields.EDUCATIONAL_CONTEXT.value,
    ],
    max_len=1000,
)

support = data.target_data[Fields.TAXONID.value].arr.sum(-2)
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
pprint({label: value for label, value in zip(data.target_data[Fields.TAXONID.value].labels, support)})
#+end_src

#+RESULTS:
#+begin_example
{'Allgemein': 64,
 'Arbeitslehre': 4,
 'Astronomie': 36,
 'Berufliche Bildung': 4,
 'Biologie': 43,
 'Chemie': 183,
 'Darstellendes Spiel': 38,
 'Deutsch': 70,
 'Deutsch als Zweitsprache': 22,
 'Elektrotechnik': 1,
 'Englisch': 62,
 'Ernährung und Hauswirtschaft': 4,
 'Ethik': 29,
 'Französisch': 47,
 'Geografie': 24,
 'Geschichte': 121,
 'Gesellschaftskunde': 7,
 'Gesundheit': 2,
 'Informatik': 78,
 'Kunst': 77,
 'MINT': 52,
 'Mathematik': 30,
 'Medienbildung': 63,
 'Mediendidaktik': 6,
 'Metalltechnik': 2,
 'Musik': 2,
 'Nachhaltigkeit': 9,
 'Open Educational Resources': 9,
 'Philosophie': 29,
 'Physik': 205,
 'Politik': 104,
 'Pädagogik': 6,
 'Religion': 18,
 'Sachunterricht': 13,
 'Spanisch': 4,
 'Sport': 17,
 'Türkisch': 5,
 'Wirtschaftskunde': 12,
 'Zeitgemäße Bildung': 1}
#+end_example

Only keep categories that have support of at least 10:
#+begin_src python
high_support = np.where(support >= 10)[0]
filtered_data = subset_categories(
    data, indices=high_support, field=Fields.TAXONID.value
)
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
filtered_support = filtered_data.target_data[Fields.TAXONID.value].arr.sum(-2)
pprint({label: value for label, value in zip(filtered_data.target_data[Fields.TAXONID.value].labels, filtered_support)})
#+end_src

#+RESULTS:
#+begin_example
{'Allgemein': 64,
 'Astronomie': 36,
 'Biologie': 43,
 'Chemie': 183,
 'Darstellendes Spiel': 38,
 'Deutsch': 70,
 'Deutsch als Zweitsprache': 22,
 'Englisch': 62,
 'Ethik': 29,
 'Französisch': 47,
 'Geografie': 24,
 'Geschichte': 121,
 'Informatik': 78,
 'Kunst': 77,
 'MINT': 52,
 'Mathematik': 30,
 'Medienbildung': 63,
 'Philosophie': 29,
 'Physik': 205,
 'Politik': 104,
 'Religion': 18,
 'Sachunterricht': 13,
 'Sport': 17,
 'Wirtschaftskunde': 12}
#+end_example

**** Example: Dropping Data with not Categories

After having dropped categories with low support, we now may have data points that do not have any assigned taxonid. Indeed, if we check, we see that multiple points have no assignments:
#+begin_src python
empty_taxonid = filtered_data.target_data[Fields.TAXONID.value].arr.sum(-1) == 0
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
print(
    "number of data points with no taxonid assignments before action:",
    empty_taxonid.sum(),
)
#+end_src

#+RESULTS:
: number of data points with no taxonid assignments before action: 12

To ensure that we only include data that actually has assignments, we can new use the ~its_data.data.subset_data_poins~ function.
#+begin_src python
filtered2_data = subset_data_points(filtered_data, np.where(~empty_taxonid)[0])
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
print(
    "number of data points with no taxonid assignments after action:",
    (filtered2_data.target_data[Fields.TAXONID.value].arr.sum(-1) == 0).sum(),
)
#+end_src

#+RESULTS:
: number of data points with no taxonid assignments after action: 0

*Important*: In order to keep the data consistent, the ~subset_data_points~ function not only modifies the metadata field we worked with, but also all other metadata fields. /This is also why we did not need to provide a field name to the function./
#+begin_src python :results output :exports results
print(
    "shape of taxonid array before filtering:",
    data.target_data[Fields.TAXONID.value].arr.shape,
)
print(
    "shape of educational context array before filtering:",
    data.target_data[Fields.EDUCATIONAL_CONTEXT.value].arr.shape,
)
print("shape of ids array before filtering:", data.ids.shape)
print("shape of test data array before filtering:", data.target_data[Fields.EDUCATIONAL_CONTEXT.value].in_test_set.shape)
print("-----------------------------------------")
print(
    "shape of taxonid array after filtering:",
    filtered2_data.target_data[Fields.TAXONID.value].arr.shape,
)
print(
    "shape of educational context array after filtering:",
    filtered2_data.target_data[Fields.EDUCATIONAL_CONTEXT.value].arr.shape,
)
print("shape of ids array after filtering:", filtered2_data.ids.shape)
print("shape of test data array after filtering:", filtered2_data.target_data[Fields.EDUCATIONAL_CONTEXT.value].in_test_set.shape)
#+end_src

#+RESULTS:
: shape of taxonid array before filtering: (924, 39)
: shape of educational context array before filtering: (924, 10)
: shape of ids array before filtering: (924,)
: shape of test data array before filtering: (924,)
: -----------------------------------------
: shape of taxonid array after filtering: (912, 24)
: shape of educational context array after filtering: (912, 10)
: shape of ids array after filtering: (912,)
: shape of test data array after filtering: (912,)

*** Sorting

It may be useful to sort the data points according to some metric, for example such that all data that has been editorially confirmed is first.
For this the ~its_data.default_pipelines.Data.subset_data_points~ method may also be used:

#+begin_src python
data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[
        Fields.TAXONID.value,
        Fields.EDUCATIONAL_CONTEXT.value,
    ],
    max_len=1000,
)

sort_indices = np.flip(np.argsort(data.editor_arr))
data_sorted = subset_data_points(data, sort_indices)
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
print("Index of first non-confirmed entry before sorting:", np.where(~data.editor_arr)[0][0])
print("Index of first non-confirmed entry after sorting:", np.where(~data_sorted.editor_arr)[0][0])
#+end_src

#+RESULTS:
: Index of first non-confirmed entry before sorting: 25
: Index of first non-confirmed entry after sorting: 870

*** Natural Language Pre-Processing and Bag of Words

The texts of generated data can additionally be pre-processed (using the [[https://github.com/openeduhub/its-prep][its_prep]] library) and transformed into bag-of-words representations:

#+begin_src python
data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[
        Fields.TAXONID.value,
        Fields.EDUCATIONAL_CONTEXT.value,
    ],
    max_len=10000,
)

processed_data = Processed_Data.from_data(data)
bow_data = BoW_Data.from_processed_data(processed_data)
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
print(f"Raw texts:\n{bow_data.raw_texts[:2]}\n")
print(f"Pre-processed texts:\n{bow_data.processed_texts[:2]}\n")
print(f"Bag-of-words shape:\n{bow_data.bows.shape}")
#+end_src

#+RESULTS:
: Raw texts:
: ['Eiszeit mit unserem neuen Freund Puck | Sport macht Spaß\nHeidewitzka, heute wird es rasant bei #SportmachtSpaß! Inga, Stefan und der Albatros laden euch in dieser Folge zum Eishockeyspielen ein, und dafür brauchen wir natürlich ein Spielgerät, nämlich den Puck. Wir zeigen euch, wie ihr den mit ein paar Materialien von zuhause selbst basteln könnt. Außerdem ist es wichtig, dass ihr ein bisschen Platz immer Zimmer schafft und gut auf Schienbeine, nackte Füße, schöne Vasen oder Haustiere aufpasst, weil die Pucks ziemlich schnell werden können. Bevor es aufs Eis geht, stärken wir uns dann erst einmal mit einem Spiel namens Bohnenfrühstück. Danach geht es aber auch schon los mit unserem Puck: Mit ihm zeichnen wir Figuren und Zahlen aufs Eis und zielen gemeinsam auf Kegel. Außerdem haben wir wieder ein Spiel fürs Wochenende im Gepäck: Es trägt den Namen Streuselkuchen und verlangt ein gutes Händchen. Seid gespannt!\n\nUnser Mitmach-Sportprogramm für Kitakinder gibt es jeden Samstag um 9 Uhr mit einer neuen Folge auf unserem YouTube-Kanal. Um 10 Uhr erscheint zudem eine Sendung „Sport macht Spaß“ für die Grundschule. \n\nWir freuen uns auf eure Ideen und Kommentare! Schreibt uns einfach eine Mail an sportmachtspass@albaberlin.de – hier könnt ihr euch auch für unseren monatlichen Newsletter mit allen Infos rund um „Sport macht Spaß“ anmelden.\n_________________ \n\nWas erwartet euch in dieser Folge? \n\n00:00 Begrüßung\n05:18 Bohnenfrühstück\n12:40 Auf die Kegel! Fertig! Schuss!\n19:54 Streuselkuchen\n__________________ \n\n„Sport macht Spaß“ ist Teil des großen digitalen Sportprogramms „Sport digital – Mehr Bewegung im Quartier“, das wir gemeinsam mit dem Bundesministerium des Innern, für Bau und Heimat (BMI) ins Leben gerufen haben. Wir bieten euch dabei kontinuierlich digitale Sportstunden für Kita- und Schulkinder hier bei YouTube an, es wird eine Mediathek mit Bildungs- und Informationsvideos aufgebaut, und es gibt Fortbildungen für pädagogische Fachkräfte im Netz und direkt in den Kiezen. Unter https://www.albaberlin.de/jugend/kita-schule-uni/sport-digital/ und https://www.miteinander-im-quartier.de/ findet ihr alle Infos zu unserem Modellprojekt, das im Rahmen der ressortübergreifenden Strategie „Soziale Stadt – Nachbarschaften stärken, miteinander im Quartier“ gefördert wird.\n\nRedaktion: Lena Flöttmann, Christoph Nicol & das ALBA-Kitateam | Kamera: Kai Rostásy & Andreas Reuther | Regie: Jörg Diernberger | Schnitt: Marcus Wojatschke\n\n#ALBA #Sportstunde #Sport #Bewegung #Zuhause #Kita #Kitasport #Kindersport #Mitmachsport #Laufspiel #Einführung #Sportgerät #Schläger #Zielen #Eishockey #Puck'
:  'Sachtexte zusammenfassen I musstewissen Deutsch\nWie fasse ich einen Sachtext zusammen? Lisa erklärt dir Schritt für Schritt, wie du dich auf die Analyse eines Sachtextes vorbereitest. \n\nWarum in die Einleitung die berühmten „W-Fragen“ gehören, der Hauptteil aus der sachlichen Wiedergabe des Textes mit eigenen Worten und ohne Wertung besteht und weshalb man im Schlussteil nochmal den Hauptgedanken aufgreift. Hat dir das Video gefallen? Dann lass gerne ein Kanal-Abo da: http://bit.ly/Deutsch_Abo \n\xa0 \nWir gehören auch zu #funk. Schaut’ da unbedingt rein:\n\nYouTube: https://youtube.com/funkofficial\nFunk Web-App: https://go.funk.net\nFacebook: https://facebook.com/funk\n\nhttps://go.funk.net/impressum']
: 
: Pre-processed texts:
: [('Eiszeit', 'mit', 'unser', 'neu', 'Freund', 'Puck', '|', 'Sport', 'machen', 'Spaß', '\n', 'Heidewitzka', '--', 'heute', 'werden', 'es', 'rasant', 'bei', '#', 'SportmachtSpaß', '--', 'Inga', '--', 'Stefan', 'und', 'der', 'Albatros', 'laden', 'euch', 'in', 'dieser', 'Folge', 'zu', 'Eishockeyspiele', 'ein', '--', 'und', 'dafür', 'brauchen', 'wir', 'natürlich', 'ein', 'Spielgerät', '--', 'nämlich', 'der', 'Puck', '--', 'wir', 'zeigen', 'euch', '--', 'wie', 'ihr', 'der', 'mit', 'ein', 'paar', 'Material', 'von', 'zuhause', 'selbst', 'basteln', 'können', '--', 'außerdem', 'sein', 'es', 'wichtig', '--', 'dass', 'ihr', 'ein', 'bisschen', 'Platz', 'immer', 'Zimmer', 'schaffen', 'und', 'gut', 'auf', 'Schienbein', '--', 'nackt', 'Fuß', '--', 'schön', 'Vase', 'oder', 'Haustier', 'aufpassen', '--', 'weil', 'der', 'Puck', 'ziemlich', 'schnell', 'werden', 'können', '--', 'bevor', 'es', 'auf', 'Eis', 'gehen', '--', 'stärken', 'wir', 'sich', 'dann', 'erst', 'einmal', 'mit', 'ein', 'Spiel', 'namens', 'Bohnenfrühstück', '--', 'danach', 'gehen', 'es', 'aber', 'auch', 'schon', 'los', 'mit', 'unser', 'Puck', '--', 'mit', 'ihm', 'zeichnen', 'wir', 'Figur', 'und', 'Zahl', 'auf', 'Eis', 'und', 'zielen', 'gemeinsam', 'auf', 'Kegel', '--', 'außerdem', 'haben', 'wir', 'wieder', 'ein', 'Spiel', 'für', 'Wochenende', 'in', 'Gepäck', '--', 'es', 'tragen', 'der', 'Name', 'Streuselkuchen', 'und', 'verlangen', 'ein', 'gut', 'Händchen', '--', 'Seid', 'gespannt', '--', '\n\n', 'Unser', 'Mitmach-Sportprogramm', 'für', 'Kitakinder', 'geben', 'es', 'jeder', 'Samstag', 'um', '9', 'Uhr', 'mit', 'ein', 'neu', 'Folge', 'auf', 'unser', 'YouTube-Kanal', '--', 'um', '10', 'Uhr', 'erscheinen', 'zudem', 'ein', 'Sendung', '--', 'Sport', 'machen', 'Spaß', '--', 'für', 'der', 'Grundschule', '--', '\n\n', 'wir', 'freuen', 'sich', 'auf', 'eur', 'Idee', 'und', 'Kommentar', '--', 'schreiben', 'uns', 'einfach', 'ein', 'Mail', 'an', 'sportmachtspass@albaberlin.de', '--', 'hier', 'können', 'ihr', 'euch', 'auch', 'für', 'unser', 'monatlich', 'Newsletter', 'mit', 'aller', 'Infos', 'rund', 'um', '--', 'Sport', 'machen', 'Spaß', '--', 'anmelden', '--', '\n', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '\n\n', 'was', 'erwarten', 'euch', 'in', 'dieser', 'Folge', '--', '\n\n', '00:00', 'Begrüßung', '\n', '05:18', 'Bohnenfrühstück', '\n', '12:40', 'auf', 'der', 'Kegel', '--', 'Fertig', '--', 'Schuss', '--', '\n', '19:54', 'Streuselkuchen', '\n', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '\n\n', '--', 'Sport', 'machen', 'Spaß', '--', 'sein', 'Teil', 'der', 'groß', 'digital', 'Sportprogramm', '--', 'Sport', 'digital', '--', 'mehr', 'Bewegung', 'in', 'Quartier', '--', '--', 'der', 'wir', 'gemeinsam', 'mit', 'der', 'Bundesministerium', 'der', 'innere', '--', 'für', 'Bau', 'und', 'Heimat', '--', 'BMI', '--', 'in', 'Leben', 'rufen', 'haben', '--', 'wir', 'bieten', 'euch', 'dabei', 'kontinuierlich', 'digital', 'Sportstunde', 'für', 'Kita', 'und', 'Schulkind', 'hier', 'bei', 'YouTube', 'an', '--', 'es', 'werden', 'ein', 'Mediathek', 'mit', 'Bildung', 'und', 'Informationsvideo', 'aufbauen', '--', 'und', 'es', 'geben', 'Fortbildung', 'für', 'pädagogisch', 'Fachkraft', 'in', 'Netz', 'und', 'direkt', 'in', 'der', 'Kiez', '--', 'unter', 'https://www.albaberlin.de/jugend/kita-schule-uni/sport-digital', '--', 'und', 'https://www.miteinander-im-quartier.de', '--', 'finden', 'ihr', 'aller', 'Infos', 'zu', 'unser', 'Modellprojekt', '--', 'der', 'in', 'Rahmen', 'der', 'ressortübergreifend', 'Strategie', '--', 'sozial', 'Stadt', '--', 'Nachbarschafte', 'stärken', '--', 'miteinander', 'in', 'Quartier', '--', 'fördern', 'werden', '--', '\n\n', 'Redaktion', '--', 'Lena', 'Flöttmann', '--', 'Christoph', 'Nicol', '&', 'der', 'ALBA-Kitateam', '|', 'Kamera', '--', 'Kai', 'Rostásy', '&', 'Andreas', 'Reuther', '|', 'Regie', '--', 'Jörg', 'Diernberger', '|', 'Schnitt', '--', 'Marcus', 'Wojatschke', '\n\n', '#', 'ALBA', '#', 'Sportstunde', '#', 'Sport', '#', 'Bewegung', '#', 'Zuhause', '#', 'Kita', '#', 'Kitasport', '#', 'Kindersport', '#', 'Mitmachsport', '#', 'Laufspiel', '#', 'Einführung', '#', 'Sportgerät', '#', 'Schläger', '#', 'Ziel', '#', 'Eishockey', '#', 'Puck'), ('Sachtext', 'zusammenfassen', 'I', 'musstewiss', 'Deutsch', '\n', 'Wie', 'fassen', 'ich', 'ein', 'Sachtext', 'zusammen', '--', 'Lisa', 'erklären', 'dir', 'Schritt', 'für', 'Schritt', '--', 'wie', 'du', 'dich', 'auf', 'der', 'Analyse', 'ein', 'Sachtext', 'vorbereitesen', '--', '\n\n', 'warum', 'in', 'der', 'Einleitung', 'der', 'berühmt', '--', 'W-Frage', '--', 'gehören', '--', 'der', 'Hauptteil', 'aus', 'der', 'sachlich', 'Wiedergabe', 'der', 'Text', 'mit', 'eigen', 'Wort', 'und', 'ohne', 'Wertung', 'bestehen', 'und', 'weshalb', 'man', 'in', 'Schlussteil', 'nochmal', 'der', 'Hauptgedanken', 'aufgreifen', '--', 'haben', 'dir', 'der', 'Video', 'fallen', '--', 'dann', 'lass', 'gerne', 'ein', 'Kanal-Abo', 'da', '--', 'http://bit.ly/Deutsch_Abo', '\n\xa0 \n', 'wir', 'gehören', 'auch', 'zu', '#', 'Funk', '--', 'Schaut', '’', 'da', 'unbedingt', 'rein', '--', '\n\n', 'YouTube', '--', 'https://youtube.com/funkofficial', '\n', 'Funk', 'Web-App', '--', 'https://go.funk.net', '\n', 'Facebook', '--', 'Https://facebook.com/funk', '\n\n', 'https://go.funk.net/impressum')]
: 
: Bag-of-words shape:
: (9343, 49911)

You may note that barely any pre-processing has occurred. This is because we did not provide any additional pre-processing pipelines. Here, we have the full flexibility of the ~its_prep~ library:

#+begin_src python
from its_prep import pipelines

more_processed_data = Processed_Data.from_data(
    data,
    pipelines.get_poc_topic_modeling_pipelines(),
)
bow_data = BoW_Data.from_processed_data(more_processed_data)
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
print("More heavily pre-processed texts:")
print(more_processed_data.processed_texts[:2])
print(f"\nBag-of-words shape:\n{bow_data.bows.shape}")
#+end_src

#+RESULTS:
: More heavily pre-processed texts:
: [('unser', 'Freund', 'Sport', 'Spaß', 'rasant', '#', 'SportmachtSpaß', 'Inga', 'Stefan', 'laden', 'Folge', 'brauchen', 'nämlich', 'paar', 'zuhause', 'basteln', 'wichtig', 'bisschen', 'Platz', 'Zimmer', 'schaffen', 'Fuß', 'schön', 'Haustier', 'ziemlich', 'schnell', 'bevor', 'auf', 'Eis', 'stärken', 'unser', 'zeichnen', 'Figur', 'Zahl', 'auf', 'Eis', 'gemeinsam', 'für', 'tragen', 'Name', 'verlangen', 'gespannt', 'Mitmach-Sportprogramm', 'Samstag', '9', 'Folge', 'unser', 'YouTube-Kanal', '10', 'erscheinen', 'zudem', 'Sendung', 'Sport', 'Spaß', 'Grundschule', 'freuen', 'eur', 'Idee', 'Kommentar', 'schreiben', 'Mail', 'sportmachtspass@albaberlin.de', 'unser', 'monatlich', 'Newsletter', 'Infos', 'Sport', 'Spaß', 'anmelden', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'erwarten', 'Folge', '00:00', 'Begrüßung', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Sport', 'Spaß', 'digital', 'Sportprogramm', 'Sport', 'digital', 'Bewegung', 'Quartier', 'gemeinsam', 'Bundesministerium', 'innere', 'Bau', 'Heimat', 'BMI', 'Leben', 'rufen', 'bieten', 'kontinuierlich', 'digital', 'Sportstunde', 'Kita', 'Schulkind', 'Mediathek', 'Bildung', 'Informationsvideo', 'aufbauen', 'Fortbildung', 'pädagogisch', 'Fachkraft', 'Netz', 'direkt', 'Kiez', 'https://www.albaberlin.de/jugend/kita-schule-uni/sport-digital', 'finden', 'Infos', 'unser', 'Modellprojekt', 'Rahmen', 'ressortübergreifend', 'Strategie', 'sozial', 'Stadt', 'Nachbarschafte', 'stärken', 'miteinander', 'Quartier', 'fördern', 'Redaktion', 'Lena', 'Flöttmann', 'Christoph', 'Nicol', 'ALBA-Kitateam', 'Kamera', 'Kai', 'Rostásy', 'Andreas', 'Reuther', 'Regie', 'Jörg', 'Schnitt', '#', 'ALBA', '#', 'Sportstunde', '#', 'Sport', '#', 'Bewegung', '#', 'Zuhause', '#', 'Kita', '#', '#', '#', '#', '#', 'Einführung', '#', '#', '#', 'Ziel', '#', '#'), ('Sachtext', 'zusammenfassen', 'I', 'musstewiss', 'Deutsch', 'fassen', 'Sachtext', 'Lisa', 'erklären', 'Analyse', 'Sachtext', 'Einleitung', 'berühmt', 'gehören', 'Hauptteil', 'Text', 'Wort', 'bestehen', 'weshalb', 'nochmal', 'aufgreifen', 'fallen', 'lass', 'gerne', 'Kanal-Abo', 'http://bit.ly/Deutsch_Abo', 'gehören', '#', 'Funk', 'Schaut', '’', 'unbedingt', 'rein', 'https://youtube.com/funkofficial', 'Funk', 'Web-App', 'https://go.funk.net', 'Facebook', 'Https://facebook.com/funk')]
: 
: Bag-of-words shape:
: (9343, 6716)

The ~its_data.default_pipelines.its_jointprobability~ pipeline already calculates bag-of-words:

#+begin_src python
from its_data.default_pipelines import its_jointprobability
bow_data = its_jointprobability.generate_data(
    json_file=Path("~/data.json"),
    target_fields=[
        Fields.TAXONID.value,
        Fields.EDUCATIONAL_CONTEXT.value,
    ],
    max_len=10000,
)
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
print(f"Raw texts:\n{bow_data.raw_texts[:2]}\n")
print(f"Pre-processed texts:\n{bow_data.processed_texts[:2]}\n")
print(f"Bag-of-words shape:\n{bow_data.bows.shape}")
#+end_src

#+RESULTS:
: Raw texts:
: ['Getrennt oder zusammen geschrieben? Lisas Deutsch-Quiz\nGetrennt- und Zusammenschreibung - für viele ein Thema zum Verzweifeln. Aber wie sieht es bei dir aus? Teste in unserem Quiz dein Wissen zur Getrennt- und Zusammenschreibung und lerne einfach die Regeln der deutschen Grammatik dazu. '
:  'Diklusive Lernwelten - Buch, Links, Beispiele, Ideen, Erfahrungen\nEin Buch über die Chancen digitaler Medien für die Inklusion wirklich aller Schülerinnen und Schüler im Unterricht.\nDen Sommer 2021 über haben 51 Autor:innen sich über die Sozialen Medien vernetzt und ein Buch über Diklusion (digitale Medien und Inklusion) geschrieben. Eingebettet in einen wissenschaftlichen Hintergrund steht es als Open Educational Ressources (OER) kostenfrei zum Download.\n\nEs ist für alle Lehrkräfte und pädagogischen Fachkräfte geeignet, die sich mit digitalen Medien im inklusiven Einsatz beschäftigen oder zukünftig befassen wollen.\n\nNoch mehr Menschen für digitale Bildung und Inklusion begeistern und damit die Chancengleichheit in der Schule unterstützen!']
: 
: Pre-processed texts:
: [('Wissen', 'schreiben', 'Lisa', 'lernen', 'Regel', 'deutsch', 'Grammatik', 'sehen', 'Test', 'Quiz'), ('Buch', 'links', 'Idee', 'Erfahrung', 'Buch', 'Chance', 'digital', 'Medium', 'Inklusion', 'Schülerin', 'Sommer', '2021', 'Autor', 'sozial', 'Medium', 'Buch', 'digital', 'Medium', 'Inklusion', 'schreiben', 'wissenschaftlich', 'Hintergrund', 'stehen', 'Educational', 'OER', 'kostenfrei', 'Download', 'Lehrkraft', 'pädagogisch', 'Fachkraft', 'eignen', 'digital', 'Medium', 'Einsatz', 'beschäftigen', 'zukünftig', 'befassen', 'Mensch', 'digital', 'Bildung', 'Inklusion', 'Schule', 'unterstützen')]
: 
: Bag-of-words shape:
: (2671, 2181)

The ~subset_data_points~ and ~subset_categories~ functions still work on ~Processed_Data~ and ~BoW_Data~. For the latter, ~subset_categories~ can even be used to modify the words included in the bag-of-words representation:

#+begin_src python
bow_data_point_subset = subset_data_points(bow_data, np.arange(100))
bow_data_words_subset = subset_categories(bow_data, np.arange(100), "bows")
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
print(f"{bow_data_point_subset.bows.shape=}")
print(f"{bow_data_words_subset.bows.shape=}")
#+end_src

#+RESULTS:
: bow_data_point_subset.bows.shape=(100, 2181)
: bow_data_words_subset.bows.shape=(2671, 100)

** Evaluation predictions

Finally, ~its_data.defaults.evaluation_functions~ contains some default functions for evaluating prediction results on certain metadata fields:

#+begin_src python
data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[
        "properties.ccm:educationalcontext",
        "properties.ccm:taxonid",
    ],
    max_len=100,
)
#+end_src

#+RESULTS:

#+begin_src python :results output
# predict True for everything
y_true = data.target_data[Fields.TAXONID.value].arr
y_pred = np.ones_like(y_true, dtype=bool)

from its_data.defaults import evaluation_functions

evaluation_functions[Fields.TAXONID.value](
    y_true,
    y_pred,
    target_names=data.target_data[Fields.TAXONID.value].labels,
)
#+end_src

#+RESULTS:
#+begin_example
                            precision    recall  f1-score   support

                   Deutsch       0.17      1.00      0.29         4
        Berufliche Bildung       0.08      1.00      0.15         2
                      MINT       0.04      1.00      0.08         1
               Philosophie       0.08      1.00      0.15         2
                  Biologie       0.04      1.00      0.08         1
                Astronomie       0.04      1.00      0.08         1
          Wirtschaftskunde       0.04      1.00      0.08         1
             Medienbildung       0.04      1.00      0.08         1
                     Kunst       0.04      1.00      0.08         1
              Arbeitslehre       0.04      1.00      0.08         1
                 Allgemein       0.04      1.00      0.08         1
                     Sport       0.04      1.00      0.08         1
                    Chemie       0.08      1.00      0.15         2
                Geschichte       0.17      1.00      0.29         4
                     Ethik       0.08      1.00      0.15         2
                    Physik       0.33      1.00      0.50         8
  Deutsch als Zweitsprache       0.04      1.00      0.08         1
Open Educational Resources       0.04      1.00      0.08         1
                   Politik       0.08      1.00      0.15         2
                Informatik       0.04      1.00      0.08         1

                 micro avg       0.08      1.00      0.15        38
                 macro avg       0.08      1.00      0.14        38
              weighted avg       0.14      1.00      0.23        38
               samples avg       0.08      1.00      0.14        38
#+end_example
