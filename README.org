:PROPERTIES:
:header-args: :results verbatim :exports both :session demo.py :async yes :var foo=imports
:END:
#+title: Data Retrieval Utilities
#+EXPORT_EXCLUDE_TAGS: noexport

A Python library that aids in retrieving and processing large data dumps from [[https://github.com/elasticsearch-dump/elasticsearch-dump][elasticdump]], primarily for the purpose of training and evaluation of AI tools for the automatic generation of metadata.

There are two primary modules of this library:
- ~data_utils.fetch~, which aids in downloading ~elasticdump~ data dumps and can turn them into ~pandas~ data frames.
  Secondarily, it implements some functionality for retrieving labels or other metadata for URIs and SKOS vocabularies.
- ~data_utils.default_pipelines~, which collects various useful pipelines, such as ~data_utils.default_pipelines.flat_classification~, that turn the data dumps into more readily usable forms whilst applying some light pre-processing.

For the purposes of downloading data dumps, this library also offers a CLI tool: ~download-data~.

For a more complete, technical documentation, see [[https://openeduhub.github.io/data-utils/]].

* Installation
:PROPERTIES:
:header-args: :results verbatim :exports both :session no 
:END:

** Nix Flakes (recommended)

When using / installing this library through ~Nix Flakes~, the only requirement is having installed [[https://nixos.org/download][Nix]] with ~Flakes~ support.

*** Running the CLI

The CLI can be run directly through ~Nix~'s CLI:
#+begin_src shell
nix run "github:openeduhub/data-utils#download-data" -- <optional arguments> <url>
#+end_src

Example:
#+begin_src shell
nix run "github:openeduhub/data-utils#download-data" -- --help
#+end_src

#+RESULTS:
#+begin_example
usage: download-data [-h] [-i INPUT_FILE] [-u USERNAME] [-p PASSWORD]
                     [-o OUTPUT_FILE] [--skip-if-exists] [--no-delete-archive]
                     [--version]
                     url

positional arguments:
  url                   The (base) URL from which to download the data dump.

options:
  -h, --help            show this help message and exit
  -i INPUT_FILE, --input-file INPUT_FILE
                        The name of the file from the URL to be downloaded. It
                        is assumed that this file is accessible through
                        <url/target-file>.
  -u USERNAME, --username USERNAME
                        The username to use when providing authentication
                        details. Optional unless a password is provided.
  -p PASSWORD, --password PASSWORD
                        The password to use when providing authentication
                        details. Optional unless a username is provided.
  -o OUTPUT_FILE, --output-file OUTPUT_FILE
                        The path to the output file. If a directory, save the
                        (decompressed) target file to this directory.
  --skip-if-exists      Skip files that already exist.
  --no-delete-archive   Do not delete the original archive if it was
                        compressed.
  --version             show program's version number and exit
#+end_example

*** As Python library, Option A: using lib

To use the library as part of a bigger Python environment, e.g. to use in another application, first include this ~Flake~ within the inputs of your own =flake.nix=:
#+begin_src nix
# flake.nix
{
  inputs = {
    data-utils.url = "/home/yusu/work/ITsJointly/projects/data_utils";
  };
}
#+end_src

Then, you can reference the Python library with the ~data-utils.lib.${system}.data-utils~ attribute. This requires a reference to the Python packages that the library shall be built with, e.g.
#+begin_src nix
# flake.nix
{
  outputs = { self, nixpkgs, ... }:
    let
      system = ...;
      pkgs = nixpkgs.legacyPackages.${system};
      my-python = pkgs.python3.withPackages (py-pkgs: [
        # example python libraries to include
        py-pkgs.pandas
        py-pkgs.numpy
        # include data-utils library
        (self.inputs.data-utils.lib.data-utils py-pkgs)
      ]);
    in
      {...};
}
#+end_src

*** As Python library, Option B: using overlays

Just like with option A, include this ~Flake~ within your own:
#+begin_src nix
# flake.nix
{
  inputs = {
    data-utils.url = "/home/yusu/work/ITsJointly/projects/data_utils";
  };
}
#+end_src

Then, when importing ~nixpkgs~, you can apply the provided overlay, which in turn allows you reference to ~data-utils~ just like any other Python library:
#+begin_src nix
# flake.nix
{
  outputs = { self, nixpkgs, ... }:
    let
      system = ...;
      pkgs = nixpkgs.legacyPackages.${system}.extend
        self.inputs.data-utils.overlays.default;
      my-python = pkgs.python3.withPackages (py-pkgs: [
        # example python libraries to include
        py-pkgs.pandas
        py-pkgs.numpy
        # include data-utils library
        py-pkgs.data-utils
      ]);
    in
      {...};
}
#+end_src


** Through pip

This library should also be installable through Python's ~pip~. Simply running ~pip install~, i.e.
#+begin_src shell
pip install <this repository>
#+end_src
should be sufficient. However, this method of installation is untested.

* Usage

** Downloading Data

The probably most common way of downloading data is through the CLI by downloading a specific file from a URL whilst specifying a username and password for authentication. This can be done through:

#+begin_src shell
download-data -u <USERNAME> -p <PASSWORD> -i <FILE_NAME> -o <OUTPUT_FILE_PATH> <URL>
#+end_src

or, by using ~nix run~:

#+begin_src shell
nix run github:openeduhub/data-utils#download-data -- -u <USERNAME> -p <PASSWORD> -i <FILE_NAME> -o <OUTPUT_FILE_PATH> <URL>
#+end_src

Alternatively, we can download the data within a Python script:
#+begin_src python
from data_utils.fetch import fetch

downloaded_file_path = fetch(
    base_url=URL,
    target_file=FILE_NAME,
    output_dir=OUTPUT_FILE_DIR, # optional
    output_file=OUTPUT_FILE_NAME, # optional
    username=USERNAME,
    password=PASSWORD,
)
#+end_src

If the target data file is detected to be compressed, i.e. its file-name ends on =.gz=, it will also be automatically be decompressed.

** Default Pipelines

The default pipelines are intended to immediately obtain all relevant, lightly pre-processed data in formats that make sense for the given information. They remove a lot of redundant work, such as converting labels into boolean arrays or pulling labels from controlled vocabularies / URIs.

Imports of modules / functions that will be used further below.
#+name: imports
#+begin_src python :var foo=""
from pathlib import Path
from pprint import pprint

import numpy as np

import data_utils.defaults as defaults
import data_utils.filters as filters
from data_utils.data import Nested_Dict, get_terminal_in
from data_utils.default_pipelines.flat_classification import generate_data
from data_utils.defaults import Fields
#+end_src

#+RESULTS: imports

*** Basic Example: Non-Hierarchical Metadata

The ~data_utils.default_pipelines.flat_classification.generate_data~ function is intended to obtain data together with any number of metadata fields that are not hierarchically organized (note that we can still apply it on hierarchical data, but the hierarchy will be discarded in the process).

After having downloaded the elasticdump json file to =~/data.json= we can directly use the ~generate_data~ function:
#+begin_src python :results silent
data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[
        "properties.ccm:educationalcontext",
        "properties.ccm:taxonid",
    ],
    max_len=1000,
)
#+end_src

The object ~data~ now contains all of the commonly used information we may need for classification tasks or their evaluation:
- =raw_texts=: The title, concatenated with the description (and separated by a =\n=).
- =ids=: The internal unique identifiers of the materials.
- =redaktion_arr=: A Boolean array containing information about whether each material belongs to the "Redaktionsbuffet" (i.e. its quality has been confirmed by an editor).
- =target_data=: Information about each selected metadata field:
  - =arr=: The Boolean matrix mapping each material to all of its relevant categories.
    When multiple assignment per material is not possible, this is equivalent to a one-hot-encoding.
  - =uris=: The URIs that correspond to each matrix column.
  - =labels=: The labels of the URIs.
  - =in_test_set=: Whether each data point belongs to the test data set for this metadatum.
    
#+begin_src python :session demo.py :exports results :results output
print(f"{data.raw_texts[0]=}\n")
print(f"{data.ids[:5]=}\n")
print(f"{data.redaktion_arr.shape=}\n")
print(f"{data.target_data.keys()=}\n")
print(f"{data.target_data['properties.ccm:taxonid'].uris[:5]=}\n")
print(f"{data.target_data['properties.ccm:taxonid'].labels[:5]=}\n")
print(f"{data.target_data['properties.ccm:taxonid'].arr.shape=}\n")
print(f"{data.target_data['properties.ccm:taxonid'].in_test_set.shape=}\n")
print(f"{data.target_data['properties.ccm:educationalcontext'].arr.shape=}\n")
#+end_src

#+RESULTS:
#+begin_example
data.raw_texts[0]='Animation zu Covid-19: Wie das Coronavirus angreift \nDieser Beitrag aus "Spektrum der Wissenschaft" erklärt in Text und Animationen, wie Sars-CoV-2 aussieht und wirkt.'

data.ids[:5]=array(['42b412c5-6aa6-45ac-bb6a-da23f231bb15',
       '48b63221-904f-4438-a6c0-37f37d98947a',
       '91aba013-36f2-4306-9b0b-540b525520f4',
       'd76d5429-efbb-4736-8c9c-25bd6569a145',
       '819a87fb-87aa-4785-aff2-c0a79c4bb2ce'], dtype=object)

data.redaktion_arr.shape=(1000,)

data.target_data.keys()=dict_keys(['properties.ccm:educationalcontext', 'properties.ccm:taxonid'])

data.target_data['properties.ccm:taxonid'].uris[:5]=['http://w3id.org/openeduhub/vocabs/discipline/020', 'http://w3id.org/openeduhub/vocabs/discipline/040', 'http://w3id.org/openeduhub/vocabs/discipline/04002', 'http://w3id.org/openeduhub/vocabs/discipline/04003', 'http://w3id.org/openeduhub/vocabs/discipline/04006']

data.target_data['properties.ccm:taxonid'].labels[:5]=['Arbeitslehre', 'Berufliche Bildung', 'Bautechnik', 'MINT', 'Ernährung und Hauswirtschaft']

data.target_data['properties.ccm:taxonid'].arr.shape=(1000, 46)

data.target_data['properties.ccm:taxonid'].in_test_set.shape=(1000,)

data.target_data['properties.ccm:educationalcontext'].arr.shape=(1000, 10)
#+end_example

In the long run, typing the full identifiers for the metadata fields can be error prone and tiring. Thus, we provide an ~Enum~ that contains the most common fields:
#+begin_src python :results output :exports results
print(f"{data.target_data[Fields.EDUCATIONAL_CONTEXT.value].uris[:5]=}\n")
#+end_src

#+RESULTS:
: data.target_data[Fields.EDUCATIONAL_CONTEXT.value].uris[:5]=['http://w3id.org/openeduhub/vocabs/educationalContext/berufliche_bildung', 'http://w3id.org/openeduhub/vocabs/educationalContext/elementarbereich', 'http://w3id.org/openeduhub/vocabs/educationalContext/erwachsenenbildung', 'http://w3id.org/openeduhub/vocabs/educationalContext/fernunterricht', 'http://w3id.org/openeduhub/vocabs/educationalContext/foerderschule']

*** Additional Options

In addition to extracting and transforming data, the default pipeline also automates various common data wrangling tasks.

**** Getting Readable Category Labels

If the values assigned to a targeted field are URIs that link back to their controlled vocabularies, the ~generate_data~ function will automatically try to look up the preferred label (default: =prefLabel.de=):

#+begin_src python
data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[Fields.TAXONID.value],
    max_len=1000,
    use_defaults=False,
)
#+end_src

#+RESULTS:

#+begin_src python :exports results :results output
pprint(
    {
        uri: label
        for uri, label in zip(
            data.target_data[Fields.TAXONID.value].uris[:10],
            data.target_data[Fields.TAXONID.value].labels,
        )
    }
)
#+end_src

#+RESULTS:
#+begin_example
{'http://w3id.org/openeduhub/vocabs/discipline/020': 'Arbeitslehre',
 'http://w3id.org/openeduhub/vocabs/discipline/040': 'Berufliche Bildung',
 'http://w3id.org/openeduhub/vocabs/discipline/04002': 'Bautechnik',
 'http://w3id.org/openeduhub/vocabs/discipline/04003': 'MINT',
 'http://w3id.org/openeduhub/vocabs/discipline/04006': 'Ernährung und '
                                                       'Hauswirtschaft',
 'http://w3id.org/openeduhub/vocabs/discipline/04009': 'Holztechnik',
 'http://w3id.org/openeduhub/vocabs/discipline/060': 'Kunst',
 'http://w3id.org/openeduhub/vocabs/discipline/080': 'Biologie',
 'http://w3id.org/openeduhub/vocabs/discipline/100': 'Chemie',
 'http://w3id.org/openeduhub/vocabs/discipline/120': 'Deutsch'}
#+end_example

Additionally, we can provide a map from metadata field to SKOS vocabulary. For all fields where this is provided, this vocabulary will be used instead of dynamically looking up the label.
This has the advantage of being much faster (only one network access instead of one per unique value) and being able to support URIs that do not directly link back to their controlled vocabularies.
#+begin_src python 
data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[Fields.TAXONID.value],
    max_len=1000,
    use_defaults=False,
    skos_urls={Fields.TAXONID.value: "https://vocabs.openeduhub.de/w3id.org/openeduhub/vocabs/discipline/index.json"},
)
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
pprint(
    {
        uri: label
        for uri, label in zip(
            data.target_data[Fields.TAXONID.value].uris[:10],
            data.target_data[Fields.TAXONID.value].labels,
        )
    }
)
#+end_src

#+RESULTS:
#+begin_example
{'http://w3id.org/openeduhub/vocabs/discipline/020': 'Arbeitslehre',
 'http://w3id.org/openeduhub/vocabs/discipline/040': 'Berufliche Bildung',
 'http://w3id.org/openeduhub/vocabs/discipline/04002': 'Bautechnik',
 'http://w3id.org/openeduhub/vocabs/discipline/04003': 'MINT',
 'http://w3id.org/openeduhub/vocabs/discipline/04006': 'Ernährung und '
                                                       'Hauswirtschaft',
 'http://w3id.org/openeduhub/vocabs/discipline/04009': 'Holztechnik',
 'http://w3id.org/openeduhub/vocabs/discipline/060': 'Kunst',
 'http://w3id.org/openeduhub/vocabs/discipline/080': 'Biologie',
 'http://w3id.org/openeduhub/vocabs/discipline/100': 'Chemie',
 'http://w3id.org/openeduhub/vocabs/discipline/120': 'Deutsch'}
#+end_example

Some controlled vocabularies are already defined in ~data_utils.defaults.skos_urls~:
#+begin_src python :results output :exports results 
pprint(defaults.skos_urls)
#+end_src

#+RESULTS:
: {'properties.ccm:educationalcontext': 'https://vocabs.openeduhub.de/w3id.org/openeduhub/vocabs/educationalContext/index.json',
:  'properties.ccm:fskRating': 'https://vocabs.openeduhub.de/w3id.org/openeduhub/vocabs/fskRating/index.json',
:  'properties.ccm:taxonid': 'https://vocabs.openeduhub.de/w3id.org/openeduhub/vocabs/discipline/index.json'}

**** Fixing Inconsistent Categories

The =remapped_values= argument allows us to provide a dictionary for any number of selected metadata fields, defining which original values shall be mapped to which new ones. For example, this may be used to unify the language codes:

#+begin_src python
example_remapped_values = {
    Fields.LANGUAGE.value: {
        "de_DE": "de",
        "de_AT": "de",
        "DE": "de",
        "de-DE": "de",
        "Deutsch": "de",
        "en-US-LEARN": "en",
        "en_US": "en",
        "en_GB": "en",
        "hu_HU": "hu",
        "es_CR": "es",
        "es_ES": "es",
        "es_AR": "es",
        "fr_FR": "fr",
        "tr_TR": "tr",
        "latin": "la",
    }
}
#+end_src

#+RESULTS:

Additionally, specific values can be dropped entirely (but not the corresponding entry) with the =dropped_values= argument, which takes a dictionary mapping metadata field to a collection of strings that shall be dropped.

Note that for some metadata fields, there already exists some defaults that may be used (see [[file:data_utils/defaults.py][defaults.py]]). These are loaded automatically when the =use_defaults= argument is set to =True= (default).

Example without defaults:
#+begin_src python
data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[Fields.TAXONID.value],
    use_defaults=False,
)
#+end_src

#+RESULTS:

#+begin_src python :exports results :results output
pprint(data.target_data[Fields.TAXONID.value].arr.shape)
pprint(data.target_data[Fields.TAXONID.value].uris)
#+end_src

#+RESULTS:
#+begin_example
(295105, 86)
['',
 'http://w3id.org/openeduhub/vocabs/discipline/020',
 'http://w3id.org/openeduhub/vocabs/discipline/040',
 'http://w3id.org/openeduhub/vocabs/discipline/04001',
 'http://w3id.org/openeduhub/vocabs/discipline/04002',
 'http://w3id.org/openeduhub/vocabs/discipline/04003',
 'http://w3id.org/openeduhub/vocabs/discipline/04005',
 'http://w3id.org/openeduhub/vocabs/discipline/04006',
 'http://w3id.org/openeduhub/vocabs/discipline/04007',
 'http://w3id.org/openeduhub/vocabs/discipline/04009',
 'http://w3id.org/openeduhub/vocabs/discipline/04011',
 'http://w3id.org/openeduhub/vocabs/discipline/04012',
 'http://w3id.org/openeduhub/vocabs/discipline/04013',
 'http://w3id.org/openeduhub/vocabs/discipline/04014',
 'http://w3id.org/openeduhub/vocabs/discipline/060',
 'http://w3id.org/openeduhub/vocabs/discipline/080',
 'http://w3id.org/openeduhub/vocabs/discipline/100',
 'http://w3id.org/openeduhub/vocabs/discipline/120',
 'http://w3id.org/openeduhub/vocabs/discipline/12002',
 'http://w3id.org/openeduhub/vocabs/discipline/160',
 'http://w3id.org/openeduhub/vocabs/discipline/20001',
 'http://w3id.org/openeduhub/vocabs/discipline/20002',
 'http://w3id.org/openeduhub/vocabs/discipline/20003',
 'http://w3id.org/openeduhub/vocabs/discipline/20004',
 'http://w3id.org/openeduhub/vocabs/discipline/20005',
 'http://w3id.org/openeduhub/vocabs/discipline/20006',
 'http://w3id.org/openeduhub/vocabs/discipline/20007',
 'http://w3id.org/openeduhub/vocabs/discipline/20008',
 'http://w3id.org/openeduhub/vocabs/discipline/20009',
 'http://w3id.org/openeduhub/vocabs/discipline/20041',
 'http://w3id.org/openeduhub/vocabs/discipline/220',
 'http://w3id.org/openeduhub/vocabs/discipline/240',
 'http://w3id.org/openeduhub/vocabs/discipline/260',
 'http://w3id.org/openeduhub/vocabs/discipline/28002',
 'http://w3id.org/openeduhub/vocabs/discipline/28010',
 'http://w3id.org/openeduhub/vocabs/discipline/320',
 'http://w3id.org/openeduhub/vocabs/discipline/340',
 'http://w3id.org/openeduhub/vocabs/discipline/380',
 'http://w3id.org/openeduhub/vocabs/discipline/400',
 'http://w3id.org/openeduhub/vocabs/discipline/420',
 'http://w3id.org/openeduhub/vocabs/discipline/440',
 'http://w3id.org/openeduhub/vocabs/discipline/44006',
 'http://w3id.org/openeduhub/vocabs/discipline/44007',
 'http://w3id.org/openeduhub/vocabs/discipline/44099',
 'http://w3id.org/openeduhub/vocabs/discipline/450',
 'http://w3id.org/openeduhub/vocabs/discipline/460',
 'http://w3id.org/openeduhub/vocabs/discipline/46014',
 'http://w3id.org/openeduhub/vocabs/discipline/480',
 'http://w3id.org/openeduhub/vocabs/discipline/48005',
 'http://w3id.org/openeduhub/vocabs/discipline/50001',
 'http://w3id.org/openeduhub/vocabs/discipline/50005',
 'http://w3id.org/openeduhub/vocabs/discipline/510',
 'http://w3id.org/openeduhub/vocabs/discipline/520',
 'http://w3id.org/openeduhub/vocabs/discipline/560',
 'http://w3id.org/openeduhub/vocabs/discipline/600',
 'http://w3id.org/openeduhub/vocabs/discipline/640',
 'http://w3id.org/openeduhub/vocabs/discipline/64018',
 'http://w3id.org/openeduhub/vocabs/discipline/660',
 'http://w3id.org/openeduhub/vocabs/discipline/680',
 'http://w3id.org/openeduhub/vocabs/discipline/700',
 'http://w3id.org/openeduhub/vocabs/discipline/720',
 'http://w3id.org/openeduhub/vocabs/discipline/72001',
 'http://w3id.org/openeduhub/vocabs/discipline/900',
 'http://w3id.org/openeduhub/vocabs/discipline/999',
 'http://w3id.org/openeduhub/vocabs/discipline/???',
 'http://w3id.org/openeduhub/vocabs/discipline/Darstellendes-Spiel',
 'http://w3id.org/openeduhub/vocabs/discipline/Deutsch',
 'http://w3id.org/openeduhub/vocabs/discipline/Deutsch als Zweitsprache',
 'http://w3id.org/openeduhub/vocabs/discipline/Englisch',
 'http://w3id.org/openeduhub/vocabs/discipline/Geografie',
 'http://w3id.org/openeduhub/vocabs/discipline/Geschichte',
 'http://w3id.org/openeduhub/vocabs/discipline/Informatik',
 'http://w3id.org/openeduhub/vocabs/discipline/Inhalte',
 'http://w3id.org/openeduhub/vocabs/discipline/Mathematik',
 'http://w3id.org/openeduhub/vocabs/discipline/Physik',
 'http://w3id.org/openeduhub/vocabs/discipline/Pädagogik',
 'http://w3id.org/openeduhub/vocabs/discipline/Religion',
 'http://w3id.org/openeduhub/vocabs/discipline/Spanisch',
 'http://w3id.org/openeduhub/vocabs/discipline/niederdeutsch',
 'http://w3id.org/openeduhub/vocabs/discipline/oeh01',
 'http://w3id.org/openeduhub/vocabs/discipline/oeh04010',
 'https://w3id.org/openeduhub/vocabs/discipline/120',
 'https://w3id.org/openeduhub/vocabs/discipline/320',
 'https://w3id.org/openeduhub/vocabs/discipline/380',
 'https://w3id.org/openeduhub/vocabs/discipline/460',
 'https://w3id.org/openeduhub/vocabs/discipline/720']
#+end_example

Example with defaults:
#+begin_src python
data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[Fields.TAXONID.value],
    use_defaults=True,
)
#+end_src

#+RESULTS:

#+begin_src python :exports results :results output
pprint(data.target_data[Fields.TAXONID.value].arr.shape)
pprint(data.target_data[Fields.TAXONID.value].uris)
#+end_src

#+RESULTS:
#+begin_example
(158292, 66)
['http://w3id.org/openeduhub/vocabs/discipline/020',
 'http://w3id.org/openeduhub/vocabs/discipline/040',
 'http://w3id.org/openeduhub/vocabs/discipline/04001',
 'http://w3id.org/openeduhub/vocabs/discipline/04002',
 'http://w3id.org/openeduhub/vocabs/discipline/04003',
 'http://w3id.org/openeduhub/vocabs/discipline/04005',
 'http://w3id.org/openeduhub/vocabs/discipline/04006',
 'http://w3id.org/openeduhub/vocabs/discipline/04007',
 'http://w3id.org/openeduhub/vocabs/discipline/04009',
 'http://w3id.org/openeduhub/vocabs/discipline/04011',
 'http://w3id.org/openeduhub/vocabs/discipline/04012',
 'http://w3id.org/openeduhub/vocabs/discipline/04013',
 'http://w3id.org/openeduhub/vocabs/discipline/04014',
 'http://w3id.org/openeduhub/vocabs/discipline/060',
 'http://w3id.org/openeduhub/vocabs/discipline/080',
 'http://w3id.org/openeduhub/vocabs/discipline/100',
 'http://w3id.org/openeduhub/vocabs/discipline/120',
 'http://w3id.org/openeduhub/vocabs/discipline/12002',
 'http://w3id.org/openeduhub/vocabs/discipline/160',
 'http://w3id.org/openeduhub/vocabs/discipline/20001',
 'http://w3id.org/openeduhub/vocabs/discipline/20002',
 'http://w3id.org/openeduhub/vocabs/discipline/20003',
 'http://w3id.org/openeduhub/vocabs/discipline/20004',
 'http://w3id.org/openeduhub/vocabs/discipline/20005',
 'http://w3id.org/openeduhub/vocabs/discipline/20006',
 'http://w3id.org/openeduhub/vocabs/discipline/20007',
 'http://w3id.org/openeduhub/vocabs/discipline/20008',
 'http://w3id.org/openeduhub/vocabs/discipline/20009',
 'http://w3id.org/openeduhub/vocabs/discipline/20041',
 'http://w3id.org/openeduhub/vocabs/discipline/220',
 'http://w3id.org/openeduhub/vocabs/discipline/240',
 'http://w3id.org/openeduhub/vocabs/discipline/260',
 'http://w3id.org/openeduhub/vocabs/discipline/28002',
 'http://w3id.org/openeduhub/vocabs/discipline/28010',
 'http://w3id.org/openeduhub/vocabs/discipline/320',
 'http://w3id.org/openeduhub/vocabs/discipline/340',
 'http://w3id.org/openeduhub/vocabs/discipline/380',
 'http://w3id.org/openeduhub/vocabs/discipline/400',
 'http://w3id.org/openeduhub/vocabs/discipline/420',
 'http://w3id.org/openeduhub/vocabs/discipline/440',
 'http://w3id.org/openeduhub/vocabs/discipline/44006',
 'http://w3id.org/openeduhub/vocabs/discipline/44007',
 'http://w3id.org/openeduhub/vocabs/discipline/44099',
 'http://w3id.org/openeduhub/vocabs/discipline/450',
 'http://w3id.org/openeduhub/vocabs/discipline/460',
 'http://w3id.org/openeduhub/vocabs/discipline/46014',
 'http://w3id.org/openeduhub/vocabs/discipline/480',
 'http://w3id.org/openeduhub/vocabs/discipline/48005',
 'http://w3id.org/openeduhub/vocabs/discipline/50001',
 'http://w3id.org/openeduhub/vocabs/discipline/50005',
 'http://w3id.org/openeduhub/vocabs/discipline/510',
 'http://w3id.org/openeduhub/vocabs/discipline/520',
 'http://w3id.org/openeduhub/vocabs/discipline/560',
 'http://w3id.org/openeduhub/vocabs/discipline/600',
 'http://w3id.org/openeduhub/vocabs/discipline/640',
 'http://w3id.org/openeduhub/vocabs/discipline/64018',
 'http://w3id.org/openeduhub/vocabs/discipline/660',
 'http://w3id.org/openeduhub/vocabs/discipline/680',
 'http://w3id.org/openeduhub/vocabs/discipline/700',
 'http://w3id.org/openeduhub/vocabs/discipline/720',
 'http://w3id.org/openeduhub/vocabs/discipline/72001',
 'http://w3id.org/openeduhub/vocabs/discipline/900',
 'http://w3id.org/openeduhub/vocabs/discipline/999',
 'http://w3id.org/openeduhub/vocabs/discipline/niederdeutsch',
 'http://w3id.org/openeduhub/vocabs/discipline/oeh01',
 'http://w3id.org/openeduhub/vocabs/discipline/oeh04010']
#+end_example

**** Filtering out Entries

In addition to modifying categories, we can also define arbitrary rules that let us drop data points before they have even been fully processed. This can be used, for example, for filtering out data that is not of sufficient quality or that does not fulfill certain conditions.

To add such rules, use the ~filters~ keyword-argument of ~generate_data~:
#+begin_src python
def my_filter(entry: Nested_Dict) -> bool:
    description = get_terminal_in(
        entry,
        Fields.DESCRIPTION.value.split("."),
    )
    if description is None:
        return False
    # the description field is multi-valued
    return len(description[0]) > 5


data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[Fields.TAXONID.value],
    max_len=1000,
    use_defaults=False,
    filters=[my_filter],
)
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
print("Minimum text length:", min(len(text) for text in data.raw_texts))
#+end_src

#+RESULTS:
: Minimum text length: 39

To simplify the process of defining such filter functions, the ~data_utils.filters~ module provides various helper functions. Especially useful here is ~get_filter_with_basic_predicate~, which creates a filter from a basic predicate function and a reference to the field to apply it to. /(Basic predicate functions are functions that map strings, floats, integers or None-values to a Boolean)./
#+begin_src python
my_filter2 = filters.get_filter_with_basic_predicate(
    lambda x: x is not None and len(x) > 5,
    Fields.DESCRIPTION.value,
    multi_value_semantics=any, # doesn't matter here; we only ever have one description
)

data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[Fields.TAXONID.value],
    max_len=1000,
    use_defaults=False,
    filters=[my_filter2],
)
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
print("Minimum text length:", min(len(text) for text in data.raw_texts))
#+end_src

#+RESULTS:
: Minimum text length: 39

For more examples on how to define filter functions, see [[file:data_utils/filters.py][data_utils.filters]].

**** Dropping Categories and Data-Points using Global Information (e.g. Support)

While the ~generate_data~ function does not directly support filtering mechanisms that rely on information that is only present once the entire data set is loaded, we provide some utility methods to easily deal with such tasks after the data has been generated.

- ~data_utils.default_pipelines.data.Data.subset_data_points~ allows for dropping or sorting data points
- ~data_utils.default_pipelines.data.Data.subset_categories~ allows for dropping or sorting categories

***** Example: Dropping Categories with low Support

Load the data and calculate the initial support:
#+begin_src python
data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[
        Fields.TAXONID.value,
        Fields.EDUCATIONAL_CONTEXT.value,
    ],
    max_len=1000,
)

support = data.target_data[Fields.TAXONID.value].arr.sum(-2)
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
pprint({label: value for label, value in zip(data.target_data[Fields.TAXONID.value].labels, support)})
#+end_src

#+RESULTS:
#+begin_example
{'Allgemein': 60,
 'Arbeitslehre': 1,
 'Astronomie': 3,
 'Bautechnik': 1,
 'Berufliche Bildung': 2,
 'Biologie': 47,
 'Chemie': 77,
 'Darstellendes Spiel': 22,
 'Deutsch': 61,
 'Deutsch als Zweitsprache': 126,
 'Englisch': 73,
 'Ernährung und Hauswirtschaft': 2,
 'Ethik': 19,
 'Französisch': 26,
 'Geografie': 18,
 'Geschichte': 120,
 'Gesellschaftskunde': 7,
 'Gesundheit': 4,
 'Holztechnik': 2,
 'Informatik': 27,
 'Interkulturelle Bildung': 2,
 'Kunst': 55,
 'MINT': 6,
 'Mathematik': 28,
 'Medienbildung': 35,
 'Mediendidaktik': 2,
 'Musik': 5,
 'Nachhaltigkeit': 42,
 'Open Educational Resources': 12,
 'Philosophie': 18,
 'Physik': 89,
 'Politik': 57,
 'Pädagogik': 8,
 'Religion': 15,
 'Sachunterricht': 9,
 'Sonderpädagogik': 2,
 'Sozialpädagogik': 1,
 'Spanisch': 152,
 'Sport': 17,
 'Türkisch': 8,
 'Umweltgefährdung, Umweltschutz': 1,
 'Weiterbildung': 1,
 'Werken': 1,
 'Wirtschaft und Verwaltung': 1,
 'Wirtschaftskunde': 15,
 'Zeitgemäße Bildung': 8}
#+end_example

Only keep categories that have support of at least 10:
#+begin_src python
high_support = np.where(support >= 10)[0]
filtered_data = data.subset_categories(
    indices=high_support, field=Fields.TAXONID.value
)
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
filtered_support = filtered_data.target_data[Fields.TAXONID.value].arr.sum(-2)
pprint({label: value for label, value in zip(filtered_data.target_data[Fields.TAXONID.value].labels, filtered_support)})
#+end_src

#+RESULTS:
#+begin_example
{'Allgemein': 60,
 'Biologie': 47,
 'Chemie': 77,
 'Darstellendes Spiel': 22,
 'Deutsch': 61,
 'Deutsch als Zweitsprache': 126,
 'Englisch': 73,
 'Ethik': 19,
 'Französisch': 26,
 'Geografie': 18,
 'Geschichte': 120,
 'Informatik': 27,
 'Kunst': 55,
 'Mathematik': 28,
 'Medienbildung': 35,
 'Nachhaltigkeit': 42,
 'Open Educational Resources': 12,
 'Philosophie': 18,
 'Physik': 89,
 'Politik': 57,
 'Religion': 15,
 'Spanisch': 152,
 'Sport': 17,
 'Wirtschaftskunde': 15}
#+end_example

***** Example: Dropping Data with not Categories

After having dropped categories with low support, we now may have data points that do not have any assigned taxonid. Indeed, if we check, we see that multiple points have no assignments:
#+begin_src python
empty_taxonid = filtered_data.target_data[Fields.TAXONID.value].arr.sum(-1) == 0
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
print(
    "number of data points with no taxonid assignments before action:",
    empty_taxonid.sum(),
)
#+end_src

#+RESULTS:
: number of data points with no taxonid assignments before action: 18

To ensure that we only include data that actually has assignments, we can new use the ~data_utils.data.subset_data_poins~ function.
#+begin_src python
filtered2_data = filtered_data.subset_data_points(
    np.where(~empty_taxonid)[0]
)
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
print(
    "number of data points with no taxonid assignments after action:",
    (filtered2_data.target_data[Fields.TAXONID.value].arr.sum(-1) == 0).sum(),
)
#+end_src

#+RESULTS:
: number of data points with no taxonid assignments after action: 0

*Important*: In order to keep the data consistent, the ~subset_data_points~ function not only modifies the metadata field we worked with, but also all other metadata fields. /This is also why we did not need to provide a field name to the function./
#+begin_src python :results output :exports results
print(
    "shape of taxonid array before filtering:",
    data.target_data[Fields.TAXONID.value].arr.shape,
)
print(
    "shape of educational context array before filtering:",
    data.target_data[Fields.EDUCATIONAL_CONTEXT.value].arr.shape,
)
print("shape of ids array before filtering:", data.ids.shape)
print("shape of test data array before filtering:", data.target_data[Fields.EDUCATIONAL_CONTEXT.value].in_test_set.shape)
print("-----------------------------------------")
print(
    "shape of taxonid array after filtering:",
    filtered2_data.target_data[Fields.TAXONID.value].arr.shape,
)
print(
    "shape of educational context array after filtering:",
    filtered2_data.target_data[Fields.EDUCATIONAL_CONTEXT.value].arr.shape,
)
print("shape of ids array after filtering:", filtered2_data.ids.shape)
print("shape of test data array after filtering:", filtered2_data.target_data[Fields.EDUCATIONAL_CONTEXT.value].in_test_set.shape)
#+end_src

#+RESULTS:
: shape of taxonid array before filtering: (1000, 46)
: shape of educational context array before filtering: (1000, 10)
: shape of ids array before filtering: (1000,)
: shape of test data array before filtering: (1000,)
: -----------------------------------------
: shape of taxonid array after filtering: (982, 24)
: shape of educational context array after filtering: (982, 10)
: shape of ids array after filtering: (982,)
: shape of test data array after filtering: (982,)

**** Sorting

It may be useful to sort the data points according to some metric, for example such that all data that has been editorially confirmed is first.
For this the ~data_utils.default_pipelines.Data.subset_data_points~ method may also be used:

#+begin_src python
data = generate_data(
    json_file=Path("~/data.json"),
    target_fields=[
        Fields.TAXONID.value,
        Fields.EDUCATIONAL_CONTEXT.value,
    ],
    max_len=1000,
)

sort_indices = np.flip(np.argsort(data.redaktion_arr))
data_sorted = data.subset_data_points(sort_indices)
#+end_src

#+RESULTS:

#+begin_src python :results output :exports results
print("Index of first non-confirmed entry before sorting:", np.where(~data.redaktion_arr)[0][0])
print("Index of first non-confirmed entry after sorting:", np.where(~data_sorted.redaktion_arr)[0][0])
#+end_src

#+RESULTS:
: Index of first non-confirmed entry before sorting: 16
: Index of first non-confirmed entry after sorting: 967
