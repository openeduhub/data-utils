from __future__ import annotations

from collections.abc import Collection, Iterable
from functools import reduce
from pathlib import Path
from typing import Optional

from its_data.default_pipelines import flat_classification
import its_data.filters as filt
import numpy as np
from its_data.default_pipelines.data import (
    BoW_Data,
    Processed_Data,
    subset_categories,
    subset_data_points,
)
from its_data.default_pipelines.extra_nlp_filters import get_repetition_filter
from its_data.defaults import Fields
from nlprep import pipelines


def generate_data(
    json_file: Path,
    target_fields: Collection[str],
    cache_dir: Optional[Path] = None,
    filters: Iterable[filt.Filter] = tuple(),
    **kwargs,
) -> BoW_Data:
    """
    Generate bag-of-words data, as used by its-jointprobability
    (i.e. supervised topic modeling).

    This builds on top of
    :func:`its_data.default_pipelines.flat_classification.generate_data`,
    applying additional filters, text-pre-processing steps,
    and data quality insurances.

    :args json_file: The path to the raw, line-separated json file,
        as generated by :func:`its_data.fetch.fetch`.
    :args target_fields: The property names to be used as targets,
        see :ref:`its_data.defaults.Fields` for an Enum containing some.
    :args cache_dir: The path to a directory to use for caching pre-processed
        texts. Primarily useful if this pipeline may be applied on identical
        texts multiple times, as this pre-processing step can take fairly long.
    :args filters: Additional filters to apply during data importing.
    :args kwargs: Additional keyword-arguments to be passed to
        :func:`its_data.default_pipelines.flat_classification.generate_data`.
    """
    print("Reading data...")
    # only keep data that contains at least one target
    labeled_filter = filt.get_labeled_filter(target_fields, multi_field_semantics=any)
    # drop materials with too little text
    text_len_filter = filt.get_len_filter(
        fields=[Fields.DESCRIPTION.value, Fields.TITLE.value],
        min_lengths=30,
        multi_field_semantics=any,
    )

    base_data = flat_classification.generate_data(
        json_file=json_file,
        target_fields=target_fields,
        filters={labeled_filter, text_len_filter} | set(filters),
        **kwargs,
    )

    # drop target categories with no label
    for target in target_fields:
        label_is_not_none = np.array(
            [label is not None for label in base_data.target_data[target].labels]
        )
        base_data = subset_categories(
            base_data, indices=np.where(label_is_not_none)[0], field=target
        )

    # pre-process texts
    # the basic pre-processing pipeline, defined in the NLP library
    my_pipelines = [
        lambda docs, **kwargs: [
            get_repetition_filter(min_rep_count=4, post_filter_count=2)
        ]
    ]

    # add a custom pipeline that combines sequences of repeated tokens.
    # in particular, combine sequences of at least three identical tokens into
    # one that is only two tokens long
    my_pipelines += list(
        pipelines.get_poc_topic_modeling_pipelines(
            ignored_upos_tags={
                "PUNCT",
                "SPACE",
                "X",
                "SCONJ",
                "PRON",
                "PART",
                "INTJ",
                "DET",
                "CCONJ",
                "AUX",
                "ADP",
            },
            required_df_interval={
                "min_num": 10,
                "max_rate": 0.25,
                "interval_open": False,
                "count_only_selected": True,
            },
        )
    )

    print("Pre-processing texts...")
    processed_data = Processed_Data.from_data(
        base_data,
        pipeline_generators=my_pipelines,
        cache_dir=cache_dir,
    )
    del base_data  # now redundant

    # filter out any documents that are not in German.
    # we do this here, rather than using any of the existing metadata, because
    # the latter are of poor quality and include many texts that are not in the
    # labeled language
    processed_data = subset_data_points(
        processed_data, np.where(processed_data.languages == "de")[0]
    )

    # calculate bag of words
    print("Transforming into bag of words...")
    bow_data = BoW_Data.from_processed_data(processed_data)
    del processed_data  # now redundant

    # drop all tokens that make up more than 0.5% of all tokens
    # keep_tokens = (bow_data.bows.sum(-2) / bow_data.bows.sum()) < 0.005
    # bow_data = subset_categories(bow_data, np.where(keep_tokens)[0], field="bows")

    # extremely long tokens are usually invalid, so drop them
    keep_tokens = np.array([len(word) <= 30 for word in bow_data.words])
    bow_data = subset_categories(bow_data, np.where(keep_tokens)[0], field="bows")

    # ensure that all docs have at least ten tokens,
    # each token is in at least five docs,
    # and each category has at least ten documents
    print("Dropping values and categories that do not fit quality criteria...")
    lens = bow_data.bows.sum(-1)
    support = (bow_data.bows > 0).sum(-2)
    target_supports = {
        target: target_data.arr.sum(-2)
        for target, target_data in bow_data.target_data.items()
    }
    while (
        lens.min() < 10
        or support.min() < 5
        or any(target_support.min() < 10 for target_support in target_supports.values())
    ):
        print(f"Shape of bag of words: {bow_data.bows.shape}")
        # drop tokens that have too small support
        bow_data = subset_categories(bow_data, np.where(support >= 5)[0], field="bows")

        # drop docs that are too small
        lens = bow_data.bows.sum(-1)
        bow_data = subset_data_points(bow_data, np.where(lens >= 10)[0])

        # drop disciplines that have too low support
        target_supports = {
            target: target_data.arr.sum(-2)
            for target, target_data in bow_data.target_data.items()
        }
        for target, target_support in target_supports.items():
            bow_data = subset_categories(
                bow_data, np.where(target_support >= 10)[0], field=target
            )

        # drop all documents that have no categories assigned
        to_keep = reduce(
            np.logical_or,
            (
                target_data.arr.sum(1) > 0
                for target_data in bow_data.target_data.values()
            ),
            np.zeros_like(bow_data.ids, dtype=bool),
        )
        bow_data = subset_data_points(bow_data, np.where(to_keep)[0])

        # re-calculate all metrics
        lens = bow_data.bows.sum(-1)
        support = (bow_data.bows > 0).sum(-2)
        target_supports = {
            target: target_data.arr.sum(-2)
            for target, target_data in bow_data.target_data.items()
        }

    # sort by editorial status, such that confirmed data is before unconfirmed
    bow_data = subset_data_points(bow_data, np.flip(np.argsort(bow_data.editor_arr)))

    return bow_data
